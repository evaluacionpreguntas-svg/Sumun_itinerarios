import streamlit as st
import pandas as pd
import vertexai
from vertexai.preview.generative_models import GenerativeModel
from vertexai.preview.vision_models import ImageGenerationModel
from vertexai.preview.generative_models import Part
import PyPDF2
import docx
import re
import io
import os
import json
import zipfile
# --- NUEVA IMPORTACI√ìN PARA GCS ---
from google.cloud import storage
import requests
from streamlit_lottie import st_lottie
from graficos_plugins import generar_grafico_desde_texto
from docx.shared import Inches
import random
from dotenv import load_dotenv
import numpy as np
from langchain_text_splitters import RecursiveCharacterTextSplitter
from vertexai.language_models import TextEmbeddingModel # Para los embeddings
import fitz  # PyMuPDF


def parse_json_llm(s: str):
    """
    Busca y decodifica un objeto JSON dentro de un string, 
    limpiando errores comunes de formato de los LLMs.
    Devuelve un diccionario si tiene √©xito, o None si no puede.
    """
    if not s:
        return None
    
    # Quitar cercos de c√≥digo tipo ```json ... ``` o ``` ... ```
    s = re.sub(r"^```(?:json)?\s*|\s*```$", "", s.strip(), flags=re.DOTALL)

    # Buscar el objeto JSON principal (del primer '{' al √∫ltimo '}')
    start = s.find('{')
    end = s.rfind('}')
    if start == -1 or end == -1:
        return None
        
    json_str = s[start:end+1]

    try:
        # Intentar decodificar
        return json.loads(json_str)
    except json.JSONDecodeError:
        # Si falla, podr√≠a estar "doble codificado" (un string JSON dentro de otro)
        try:
            decoded_str = json.loads(f'"{json_str}"')
            return json.loads(decoded_str)
        except Exception:
            return None

def load_bloom_taxonomy(file_path="bloom_taxonomy.json"):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        st.error(f"Error: No se encontr√≥ el archivo de la taxonom√≠a en '{file_path}'.")
        return {}
    except json.JSONDecodeError:
        st.error(f"Error: El archivo '{file_path}' no es un JSON v√°lido.")
        return {}

# Carga la taxonom√≠a al inicio de tu script
bloom_taxonomy_detallada = load_bloom_taxonomy()

# --- REEMPLAZA ESTA FUNCI√ìN ---
def crear_indice_vectorial(paginas_texto):
    """
    Convierte una lista de TEXTOS DE P√ÅGINA en chunks y vectores.
    Procesa p√°gina por p√°gina para ahorrar RAM.
    """
    try:
        model = TextEmbeddingModel.from_pretrained("text-embedding-004")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100
        )
        
        index = []
        
        # --- AQU√ç EST√Å EL CAMBIO ---
        # Reducimos el tama√±o del lote de 250 a 100.
        # Esto asegura que no superemos el l√≠mite de 20k tokens por llamada.
        api_batch_size = 100 
        # --- FIN DEL CAMBIO ---

        # Iteramos sobre CADA P√ÅGINA individualmente
        for texto_pagina in paginas_texto:
            # 1. Dividimos solo el texto de ESTA p√°gina
            chunks_pagina = text_splitter.split_text(texto_pagina)
            
            if not chunks_pagina:
                continue
            
            # 2. Vectorizamos los chunks de ESTA p√°gina (en lotes si es necesario)
            for i in range(0, len(chunks_pagina), api_batch_size):
                batch_chunks = chunks_pagina[i:i + api_batch_size]
                embeddings = model.get_embeddings(batch_chunks)
                
                for chunk, embedding in zip(batch_chunks, embeddings):
                    index.append((chunk, np.array(embedding.values)))
            
            # 3. Al final de este bucle, 'texto_pagina' y 'chunks_pagina' se liberan
            # de la memoria antes de procesar la siguiente p√°gina.
    
        return index
    
    except Exception as e:
        # ¬°Este es el error que est√°s viendo ahora!
        st.error(f"Error al crear vectores (Embeddings): {e}")
        return []

def buscar_en_indice(query_text, k=3):
        """
        Busca en el √≠ndice de sesi√≥n los k chunks m√°s relevantes para un texto.
        Devuelve una lista de los textos (chunks) encontrados.
        """
        if 'pdf_index' not in st.session_state or not st.session_state['pdf_index']:
            return [] # No hay √≠ndice cargado

        index = st.session_state['pdf_index']
        
        try:
            # 1. Vectorizar la consulta (la microhabilidad)
            model = TextEmbeddingModel.from_pretrained("text-embedding-004")
            query_vector = np.array(model.get_embeddings([query_text])[0].values)
            
            # 2. Calcular similitud (Coseno)
            # Preparamos los vectores del √≠ndice
            chunk_vectors = np.array([item[1] for item in index])
            
            # Normalizamos vectores
            query_norm = np.linalg.norm(query_vector)
            chunk_norms = np.linalg.norm(chunk_vectors, axis=1)
            
            # Evitar divisi√≥n por cero si hay vectores nulos
            if query_norm == 0 or np.any(chunk_norms == 0):
                return []
                
            # Calculamos la similitud del coseno
            # (A . B) / (||A|| * ||B||)
            similitudes = np.dot(chunk_vectors, query_vector) / (chunk_norms * query_norm)
            
            # 3. Obtener los Top K
            # `np.argsort` da los √≠ndices de menor a mayor. Usamos `[-k:]` para los k m√°s altos
            # y `[::-1]` para invertirlos (del m√°s al menos relevante).
            top_k_indices = np.argsort(similitudes)[-k:][::-1]
            
            # 4. Devolver los textos de esos chunks
            relevant_chunks = [index[i][0] for i in top_k_indices]
            return relevant_chunks

        except Exception as e:
            st.warning(f"Error al buscar en el √≠ndice del PDF: {e}")
            return []
    

def load_lottieurl(url: str):
    r = requests.get(url)
    if r.status_code != 200:
        return None
    return r.json()

def _pick(d, *keys):
    for k in keys:
        v = d.get(k)
        if v not in (None, ""):
            return v
    return None
    
def normaliza_claves_classif(c: dict) -> dict:
    if not c:
        return {}
    return {
        'ID': _pick(c, 'ID', 'Id', 'id'),
        'GRADO': _pick(c, 'GRADO', 'Grado', 'grado'),
        '√ÅREA': _pick(c, '√ÅREA', '√Årea', 'Area', '√°rea', 'area'),
        'ASIGNATURA': _pick(c, 'ASIGNATURA', 'Asignatura', 'asignatura'),
        'MACROHABILIDAD': _pick(c, 'MACROHABILIDAD', 'Macrohabilidad', 'Macrohabilidad', 'macrohabilidad', 'macrohabilidad'),
        'PROCESO COGNITIVO': _pick(c, 'PROCESO COGNITIVO', 'Proceso Cognitivo', 'proceso cognitivo'),
        'MICROHABILIDAD': _pick(c, 'MICROHABILIDAD', 'Microhabilidad'),
        'COMPETENCIA MICROHABILIDAD': _pick(c, 'COMPETENCIA MICROHABILIDAD', 'Competencia Microhabilidad'),
        'Numero': _pick(c, 'Numero', 'NUMERO', 'N√∫mero', 'N√öMERO', 'numero', 'n√∫mero'),
    }

def describir_imagen_con_llm(model_name, image_bytes, file_type):
    """
    Usa un modelo multimodal de Vertex AI para generar una descripci√≥n detallada de una imagen.
    """
    try:
        # Aseg√∫rate de usar un modelo que soporte visi√≥n, como gemini-1.5-pro
        model = GenerativeModel(model_name)
        
        # Prepara la imagen para el modelo
        image_part = Part.from_data(data=image_bytes, mime_type=file_type)

        # Prompt espec√≠fico para obtener una descripci√≥n √∫til para evaluaciones
        prompt_descripcion = """
        Describe esta imagen con el m√°ximo nivel de detalle posible, como si se la estuvieras describiendo a alguien que no puede verla y necesita construir una pregunta de evaluaci√≥n sobre ella.
        Enf√≥cate en los siguientes aspectos:
        1.  **Objetos y Entidades:** Lista todos los objetos, personas o animales presentes.
        2.  **Acciones y Relaciones:** Describe qu√© est√° ocurriendo y c√≥mo interact√∫an los elementos entre s√≠.
        3.  **Texto y S√≠mbolos:** Transcribe cualquier texto visible, n√∫meros, etiquetas o s√≠mbolos importantes.
        4.  **Composici√≥n y Contexto:** Describe la escena general, la disposici√≥n de los elementos y cualquier inferencia obvia sobre el lugar o la situaci√≥n.
        Genera una descripci√≥n completa en un √∫nico p√°rrafo.
        """
        
        # Genera el contenido
        response = model.generate_content([prompt_descripcion, image_part])
        return response.text
    except Exception as e:
        st.error(f"Error al describir la imagen con Vertex AI: {e}")
        return None


def extraer_texto_pdf(pdf_bytes):
    """
    Extrae texto de un PDF en bytes USANDO PyMuPDF (fitz),
    que es mucho m√°s r√°pido y robusto que PyPDF2.
    """
    try:
        texto_pdf = ""
        # Abrir el PDF desde los bytes en memoria
        with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
            for page in doc:
                # .get_text() es el m√©todo de PyMuPDF
                texto_pdf += page.get_text() + "\n\n" 
        return texto_pdf
    except Exception as e:
        # A√∫n mantenemos el st.error por si el PDF est√° da√±ado
        st.error(f"Error al leer el PDF con PyMuPDF: {e}")
        return None



# --- FUNCI√ìN PRINCIPAL QUE ENVUELVE TODA LA APP ---
def main():
    # --- CONFIGURACI√ìN DE LA P√ÅGINA DE STREAMLIT ---
    st.set_page_config(
        page_title="Generador y Auditor de √çtems con IA (Vertex AI)",
        page_icon="üß†",
        layout="wide"
    )

    # --- INICIALIZACI√ìN DE VERTEX AI ---
    try:
        GCP_PROJECT_ID = os.environ.get("GCP_PROJECT_ID")
        GCP_LOCATION = os.environ.get("GCP_LOCATION")
        vertexai.init(project=GCP_PROJECT_ID, location=GCP_LOCATION)
        st.sidebar.success("‚úÖ Conectado a Vertex AI.")
    except Exception as e:
        st.sidebar.error(f"Error al inicializar Vertex AI: {e}")
        st.error("No se pudo conectar con Vertex AI. Verifica la configuraci√≥n del proyecto y la autenticaci√≥n.")
        st.stop()

    # --- T√≠tulo Principal de la App ---
    st.title("üìö Generador y Auditor de √≠tems final de itinerario SUMUN üß†")
    st.markdown("Esta aplicaci√≥n genera √≠tems de selecci√≥n m√∫ltiple y audita su calidad utilizando modelos de **Google Cloud Vertex AI**.")


    definiciones_tipologias = {
        "Cr√≥nica": "Un relato narrativo y detallado de hechos, usualmente presentados en orden cronol√≥gico. Debe incluir la perspectiva y estilo subjetivo del autor, mezclando informaci√≥n con impresiones personales.",
        "Noticia": "Un texto objetivo y conciso que informa sobre un evento reciente y de inter√©s p√∫blico. Debe responder a las preguntas qu√©, qui√©n, c√≥mo, cu√°ndo, d√≥nde y por qu√©. El lenguaje debe ser formal y directo.",
        "Entrevista": "Un texto que presenta un di√°logo entre un entrevistador y un entrevistado. Debe estar en formato de pregunta y respuesta (Ej: 'Entrevistador: ...', 'Entrevistado: ...') y revelar informaci√≥n u opiniones del entrevistado.",
        "Ensayo": "Un texto en prosa que analiza, interpreta o eval√∫a un tema desde una perspectiva personal y argumentativa. Debe presentar una tesis clara y desarrollarla con argumentos y reflexiones.",
        "Cuento Corto": "Un relato de ficci√≥n breve, con pocos personajes y una trama concisa que se desarrolla hacia un cl√≠max y un final. Debe tener elementos narrativos claros como inicio, nudo y desenlace.",
        "Manual": "Un texto instructivo y funcional que explica paso a paso c√≥mo realizar una tarea o usar un producto. El lenguaje debe ser claro, preciso y directo, a menudo usando listas numeradas o vi√±etas."
    }
    
    # -------------------------------------------------------------------
    # --- SECCI√ìN DE DEFINICI√ìN DE TODAS LAS FUNCIONES DE AYUDA ---
    # -------------------------------------------------------------------
    
    ### INICIO DE NUEVAS FUNCIONALIDADES DE AUTOGUARDADO ###
    def generar_nombre_archivo_progreso(grado, asignatura, macrohabilidad):
        """Crea un nombre de archivo seguro y √∫nico basado en las selecciones."""
        grado_str = str(grado).replace(" ", "_")
        asignatura_str = str(asignatura).replace(" ", "_")
        macrohabilidad_str = str(macrohabilidad).replace(" ", "_")
        nombre_base = f"progreso_{grado_str}_{asignatura_str}_{macrohabilidad_str}"
        nombre_seguro = re.sub(r'[^a-zA-Z0-9_.-]', '', nombre_base)
        return f"{nombre_seguro}.json"

    def guardar_progreso_en_gcs(bucket_name, file_name, data):
        """Guarda el estado de la sesi√≥n en un archivo JSON en GCS."""
        if not bucket_name: return
        try:
            storage_client = storage.Client()
            bucket = storage_client.bucket(bucket_name)
            blob = bucket.blob(f"progreso/{file_name}") 
            json_data = json.dumps(data, indent=4)
            blob.upload_from_string(json_data, content_type='application/json')
        except Exception as e:
            st.sidebar.warning(f"No se pudo autoguardar el progreso: {e}")

    def cargar_progreso_desde_gcs(bucket_name, file_name):
        """Carga el estado de la sesi√≥n desde GCS, si existe."""
        if not bucket_name: return []
        try:
            storage_client = storage.Client()
            bucket = storage_client.bucket(bucket_name)
            blob = bucket.blob(f"progreso/{file_name}")
            if blob.exists():
                json_data = blob.download_as_string()
                data = json.loads(json_data)
                st.sidebar.success(f"Progreso recuperado para esta macrohabilidad.")
                return data
            return []
        except Exception as e:
            st.sidebar.error(f"Error al cargar el progreso: {e}")
            return []

    def borrar_progreso_en_gcs(bucket_name, file_name):
        """Borra el archivo de progreso de GCS al reiniciar."""
        if not bucket_name: return
        try:
            storage_client = storage.Client()
            bucket = storage_client.bucket(bucket_name)
            blob = bucket.blob(f"progreso/{file_name}")
            if blob.exists():
                blob.delete()
                st.sidebar.info("El progreso guardado ha sido eliminado.")
        except Exception as e:
            st.sidebar.warning(f"No se pudo borrar el progreso: {e}")
    ### FIN DE NUEVAS FUNCIONALIDADES ###
    
    @st.cache_data
    def leer_excel_desde_gcs(bucket_name, file_path):
        """
        Lee un archivo Excel directamente desde un bucket de GCS.
        """
        try:
            storage_client = storage.Client()
            bucket = storage_client.bucket(bucket_name)
            blob = bucket.blob(file_path)
            
            file_bytes = blob.download_as_bytes()
            
            df = pd.read_excel(io.BytesIO(file_bytes))
            st.sidebar.success(f"Archivo Excel '{file_path}' cargado desde GCS.")
            return df
        except Exception as e:
            st.sidebar.error(f"Error al leer Excel desde GCS: {e}")
            st.error(f"No se pudo cargar el archivo Excel desde el bucket '{bucket_name}'. Revisa los permisos y la ruta del archivo.")
            return None

    @st.cache_data
    def leer_pdf_desde_gcs(bucket_name, file_path):
        """
        Lee el texto de un archivo PDF directamente desde un bucket de GCS.
        """
        try:
            storage_client = storage.Client()
            bucket = storage_client.bucket(bucket_name)
            blob = bucket.blob(file_path)
            
            file_bytes = blob.download_as_bytes()
            
            texto_pdf = ""
            reader = PyPDF2.PdfReader(io.BytesIO(file_bytes))
            for page in reader.pages:
                texto_pdf += page.extract_text() or ""
            
            st.sidebar.success(f"Archivo PDF '{file_path}' cargado desde GCS.")
            return texto_pdf
        except Exception as e:
            st.sidebar.error(f"Ocurri√≥ un error al leer el PDF desde GCS: {e}")
            st.error(f"No se pudo cargar el archivo PDF desde el bucket '{bucket_name}'. Revisa los permisos y la ruta del archivo.")
            return ""

    # --- C√ìDIGO DE REEMPLAZO ---
    
    def generar_contexto_general_con_llm(model_name, grado, area, asignatura, macrohabilidad, tipo_contexto="", idea_usuario=""):
        """
        Genera un texto de contexto general para una macrohabilidad, aplicando una tipolog√≠a textual espec√≠fica si se proporciona.
        """
        # Construimos una secci√≥n especial para la tipolog√≠a solo si se especifica una
        seccion_tipologia = ""
        if tipo_contexto in definiciones_tipologias:
            definicion = definiciones_tipologias[tipo_contexto]
            seccion_tipologia = f"""
    --- TIPOLOG√çA TEXTUAL OBLIGATORIA ---
    ¬°INSTRUCCI√ìN CR√çTICA! El texto generado DEBE corresponder fielmente a la siguiente tipolog√≠a:
    - TIPO: {tipo_contexto}
    - DEFINICI√ìN: {definicion}
    Garantiza que la estructura, el estilo y el lenguaje del texto cumplan con esta definici√≥n.
    ------------------------------------
    """
    
        prompt_contexto = f'''Eres un escritor y dise√±ador instruccional experto. Tu tarea es redactar un texto de CONTEXTO para una evaluaci√≥n educativa.
    
    --- PAR√ÅMETROS GENERALES ---
    - Grado: {grado}
    - √Årea: {area}
    - Asignatura: {asignatura}
    - Macrohabilidad o unidad tem√°tica: {macrohabilidad}
    {seccion_tipologia}
    --- IDEA GU√çA DEL USUARIO (Opcional) ---
    {idea_usuario if idea_usuario else "No se proporcion√≥ una idea espec√≠fica, usa tu creatividad para el tema."}
    ------------------------------------
    
    --- INSTRUCCIONES FINALES ---
    - El texto debe ser coherente, veros√≠mil y apropiado para el nivel educativo.
    - Si se especific√≥ una tipolog√≠a, C√öMPLELA rigurosamente.
    - El texto final debe tener entre 150 y 300 palabras.
    - Devuelve √öNICAMENTE el texto del contexto. NO incluyas t√≠tulos (a menos que la tipolog√≠a lo requiera, como en una noticia), preguntas, ni explicaciones.
    '''
        try:
            modelo = GenerativeModel(model_name)
            response = modelo.generate_content(prompt_contexto)
            return response.text
        except Exception as e:
            st.error(f"Error al generar el contexto con Vertex AI: {e}")
            return None
    
    def refinar_contexto_con_llm(model_name, contexto_original, feedback_usuario):
        """
        Refina un texto de contexto existente basado en el feedback del usuario.
        """
        prompt_refinamiento = f"""
    Eres un editor experto. Tu tarea es reescribir y mejorar el siguiente texto de CONTEXTO basado en las observaciones del usuario.
    No cambies la intenci√≥n original del texto a menos que el feedback te lo pida. El objetivo es ajustar y perfeccionar.
    
    --- TEXTO ORIGINAL ---
    {contexto_original}
    --------------------
    
    --- OBSERVACIONES DEL USUARIO PARA REFINAR ---
    {feedback_usuario}
    -------------------------------------------
    
    --- INSTRUCCIONES ---
    - Devuelve √∫nicamente el texto del contexto refinado.
    - No a√±adas explicaciones, saludos ni ning√∫n otro texto fuera del contexto en s√≠.
    """
        try:
            modelo = GenerativeModel(model_name)
            response = modelo.generate_content(prompt_refinamiento)
            return response.text
        except Exception as e:
            st.error(f"Error al refinar el contexto con Vertex AI: {e}")
            return None
            
    def get_descripcion_detallada_bloom(proceso_cognitivo_elegido):
        """
        Busca el proceso cognitivo en la estructura detallada y devuelve una descripci√≥n formateada.
        """
        proceso_upper = str(proceso_cognitivo_elegido).upper()
        proceso_data = bloom_taxonomy_detallada.get(proceso_upper)

        if not proceso_data:
            return "Descripci√≥n no disponible para este proceso cognitivo."

        descripcion_formateada = f"**Categor√≠a Cognitiva: {proceso_upper}**\n"
        descripcion_formateada += f"- **Definici√≥n General**: {proceso_data['definicion']}\n\n"
        descripcion_formateada += "**Subprocesos Cognitivos Asociados:**\n"

        for subproceso, detalles in proceso_data['subprocesos'].items():
            descripcion_formateada += f"- **{subproceso}** (Nombres alternativos: {detalles['nombres_alternativos']}):\n"
            descripcion_formateada += f"  - {detalles['definicion_ejemplo']}\n"

        return descripcion_formateada
        
    def get_preguntas_tipo_formateadas(proceso_cognitivo_elegido):
        """
        Busca un proceso cognitivo y devuelve un string formateado 
        con todas las preguntas tipo de sus subprocesos.
        """
        proceso_upper = str(proceso_cognitivo_elegido).upper()
        proceso_data = bloom_taxonomy_detallada.get(proceso_upper)
    
        if not proceso_data or 'subprocesos' not in proceso_data:
            return "No se encontraron ejemplos de preguntas para este proceso."
    
        texto_formateado = ""
        for subproceso, detalles in proceso_data['subprocesos'].items():
            if 'Preguntas_tipo' in detalles and detalles['Preguntas_tipo']:
                texto_formateado += f"\nPara el subproceso '{subproceso}':\n"
                for pregunta in detalles['Preguntas_tipo']:
                    texto_formateado += f"- {pregunta}\n"
        
        return texto_formateado
            
    
    def generar_texto_con_llm(model_name, prompt, force_json=False):
        """
        Genera texto usando un modelo de Vertex AI, con opci√≥n para forzar salida JSON.
        """
        try:
            modelo = GenerativeModel(model_name)
            gen_config = {}
            if force_json:
                gen_config["response_mime_type"] = "application/json"
                
            response = modelo.generate_content(prompt, generation_config=gen_config)
            return response.text
        except Exception as e:
            st.error(f"Error al llamar a Vertex AI ({model_name}): {e}")
            return None

    def auditar_item_con_llm(model_name, item_generado, grado, area, asignatura, macrohabilidad,
                             proceso_cognitivo, microhabilidad,
                             competencia_microhabilidad, contexto_educativo, manual_reglas_texto="", 
                             descripcion_bloom="", grafico_necesario="", descripcion_grafico="", 
                             prompt_auditor_adicional=""):
        """
        Audita un √≠tem generado para verificar su cumplimiento con criterios espec√≠ficos,
        solicitando y esperando una respuesta en formato JSON.
        """
        auditoria_prompt = f"""
    Eres un experto en validaci√≥n de √≠tems educativos, especializado en pruebas tipo ICFES y las directrices del equipo IMPROVE.
    Tu tarea es AUDITAR RIGUROSAMENTE el siguiente √≠tem generado por un modelo de lenguaje y devolver tu an√°lisis en formato JSON.
    Debes verificar que el √≠tem cumpla con TODOS los siguientes criterios, prestando especial atenci√≥n a la alineaci√≥n con los par√°metros proporcionados y a las reglas de formato y contenido.
    
    --- CRITERIOS DE AUDITOR√çA ---
    Eval√∫a el √≠tem de manera rigurosa seg√∫n los siguientes puntos clave. La calidad de tu auditor√≠a depende de la atenci√≥n a cada detalle.
    
    1.  **Formato del Enunciado:** La pregunta debe ser clara, directa, sin ambig√ºedades ni errores gramaticales.
    
    2.  **Estilo del Enunciado (Regla de No Jerarquizaci√≥n):** ¬°CRITERIO CR√çTICO! Verifica que el enunciado no contenga palabras comparativas o superlativas que obliguen al estudiante a jerarquizar las opciones (ej. "m√°s", "mejor", "principal"). La violaci√≥n de esta regla es un error cr√≠tico y debe resultar en un **RECHAZO** autom√°tico en el dictamen final.
    
    3.  **N√∫mero de Opciones:** Debe haber exactamente 4 opciones (A, B, C, D).
    
    4.  **Respuesta Correcta Indicada:** La secci√≥n 'RESPUESTA CORRECTA:' debe estar presente, claramente indicada y coincidir con una de las opciones.
    
    5.  **Dise√±o de Justificaciones:** Deben existir justificaciones diferenciadas para cada opci√≥n. La de la opci√≥n correcta debe explicar el razonamiento (no por descarte), y las de las incorrectas deben seguir el formato: ‚ÄúEl estudiante podr√≠a escoger‚Ä¶ Sin embargo, esto es incorrecto porque‚Ä¶‚Äù.
    
    6.  **Estilo y Restricciones:** El √≠tem debe evitar negaciones mal redactadas, nombres/marcas/lugares reales, datos personales y frases vagas como ‚Äúninguna de las anteriores‚Äù o ‚Äútodas las anteriores‚Äù.
    
    7.  **Alineaci√≥n del Contenido:** Eval√∫a si el √≠tem se alinea EXCLUSIVAMENTE con todos los par√°metros:
        * **Tem√°ticos:** Grado (`{grado}`), √Årea (`{area}`), Asignatura (`{asignatura}`), Macrohabilidad (`{macrohabilidad}`), Microhabilidad (`{microhabilidad}`).
        * **Cognitivos:** Proceso (`{proceso_cognitivo}`). Crucialmente, verifica la **exclusividad cognitiva**: la tarea debe ser demostrablemente m√°s compleja que el nivel cognitivo anterior y no debe requerir un nivel superior.
    
    8.  **Gr√°fico (si aplica):** Si se requiere un gr√°fico (`{grafico_necesario}`), la descripci√≥n (`{descripcion_grafico}`) debe ser clara y funcional.
    
    --- MANUAL DE REGLAS ADICIONAL ---
    {manual_reglas_texto}
    -----------------------------------
    
    --- INSTRUCCIONES ADICIONALES PARA LA AUDITOR√çA ---
    {prompt_auditor_adicional if prompt_auditor_adicional else "No se proporcionaron instrucciones adicionales para la auditor√≠a."}
    ---------------------------------------------------
    
    --- √çTEM A AUDITAR ---
    {item_generado}
    --------------------
    
    ¬°INSTRUCCI√ìN CR√çTICA DE SALIDA!
    Devuelve tu auditor√≠a como un √∫nico bloque de c√≥digo JSON v√°lido, sin ning√∫n otro texto o explicaci√≥n antes o despu√©s. No uses ```json.
    El objeto JSON debe tener la siguiente estructura exacta, incluyendo un objeto por cada uno de los 8 criterios:
    {{
      "criterios": [
        {{
          "criterio": "Formato del Enunciado",
          "estado": "‚úÖ",
          "comentario": "El enunciado es claro y directo."
        }},
        {{
          "criterio": "Estilo del Enunciado (Regla de No Jerarquizaci√≥n)",
          "estado": "‚úÖ",
          "comentario": "El enunciado no utiliza t√©rminos comparativos."
        }},
        {{
          "criterio": "N√∫mero de Opciones",
          "estado": "‚úÖ",
          "comentario": "Se presentan 4 opciones."
        }},
        {{
          "criterio": "Respuesta Correcta Indicada",
          "estado": "‚úÖ",
          "comentario": "La clave de respuesta est√° presente y es v√°lida."
        }},
        {{
          "criterio": "Dise√±o de Justificaciones",
          "estado": "‚úÖ",
          "comentario": "Las justificaciones siguen el formato requerido."
        }},
        {{
          "criterio": "Estilo y Restricciones",
          "estado": "‚úÖ",
          "comentario": "El √≠tem no contiene nombres propios, marcas u otras restricciones."
        }},
        {{
          "criterio": "Alineaci√≥n del Contenido",
          "estado": "‚úÖ",
          "comentario": "El √≠tem se alinea correctamente con los par√°metros tem√°ticos y cognitivos."
        }},
        {{
          "criterio": "Gr√°fico (si aplica)",
          "estado": "‚úÖ",
          "comentario": "No se requiere gr√°fico, lo cual es correcto para este √≠tem."
        }}
      ],
      "dictamen_final": "‚úÖ CUMPLE TOTALMENTE",
      "observaciones_finales": "El √≠tem cumple con todos los criterios de auditor√≠a y se considera apto."
    }}
    """
        return generar_texto_con_llm(model_name, auditoria_prompt, force_json=True), auditoria_prompt

               
    def generar_pregunta_con_seleccion(gen_model_name, audit_model_name,
                                         fila_datos, criterios_generacion, manual_reglas_texto="",
                                         informacion_adicional_usuario="",
                                         prompt_bloom_adicional="", prompt_construccion_adicional="", prompt_especifico_adicional="",
                                         prompt_auditor_adicional="",
                                         contexto_general_macrohabilidad="", contexto_del_libro="",  feedback_usuario="", item_a_refinar_text="", descripcion_imagen_aprobada=""):
        """
        Genera una pregunta educativa de opci√≥n m√∫ltiple usando el modelo de generaci√≥n seleccionado
        y la itera para refinarla si la auditor√≠a lo requiere.
        """
        tipo_pregunta = criterios_generacion.get("tipo_pregunta", "opci√≥n m√∫ltiple con 4 opciones")
        dificultad = criterios_generacion.get("dificultad", "media")
        contexto_educativo = criterios_generacion.get("contexto_educativo", "general")
        formato_justificacion = criterios_generacion.get("formato_justificacion", """
            ‚Ä¢ Justificaci√≥n correcta: debe explicar el razonamiento o proceso cognitivo (NO por descarte).
            ‚Ä¢ Justificaciones incorrectas: deben redactarse como: ‚ÄúEl estudiante podr√≠a escoger la opci√≥n X porque‚Ä¶ Sin embargo, esto es incorrecto porque‚Ä¶‚Äù
        """)
        fila_datos = normaliza_claves_classif(fila_datos)
        grado_elegido = fila_datos.get('GRADO', 'no especificado')
        area_elegida = fila_datos.get('√ÅREA', 'no especificada')
        asignatura_elegida = fila_datos.get('ASIGNATURA', 'no especificada')
        macrohabilidad_elegida = fila_datos.get('MACROHABILIDAD', 'no especificada')
        proceso_cognitivo_elegido = fila_datos.get('PROCESO COGNITIVO', 'no especificado')
        microhabilidad_elegida = fila_datos.get('MICROHABILIDAD', 'no especificada')
        competencia_microhabilidad_elegida = fila_datos.get('COMPETENCIA MICROHABILIDAD', 'no especificada')
    
        dato_para_pregunta_foco = microhabilidad_elegida
        descripcion_bloom = get_descripcion_detallada_bloom(proceso_cognitivo_elegido)
        ejemplos_preguntas = get_preguntas_tipo_formateadas(proceso_cognitivo_elegido)
        
        current_item_text = ""
        auditoria_status = "‚ùå RECHAZADO"
        audit_observations = ""
        max_refinement_attempts = 3
        attempt = 0
        grafico_necesario = "NO"
        descripcion_grafico = ""
        numero_fila_elegido = fila_datos.get('Numero', 'N/A')
                                            
        classification_details = {
            "ID": fila_datos.get("ID"),
            "GRADO": grado_elegido,
            "√ÅREA": area_elegida,
            "ASIGNATURA": asignatura_elegida,
            "MACROHABILIDAD": macrohabilidad_elegida,
            "PROCESO COGNITIVO": proceso_cognitivo_elegido,
            "MICROHABILIDAD": microhabilidad_elegida,
            "COMPETENCIA MICROHABILIDAD": competencia_microhabilidad_elegida,
            "Numero": numero_fila_elegido
        }
    
        item_final_data = None
        full_generation_prompt = ""
        full_auditor_prompt = ""
    
        if feedback_usuario and item_a_refinar_text:
            prompt_refinamiento = f"""
            --- TAREA DE REFINAMIENTO ---
            Eres un experto en √≠tems de evaluaci√≥n. Tu tarea es REFINAR el siguiente √≠tem, corrigiendo o ajustando el texto para abordar las observaciones del usuario. No lo reescribas completamente; solo haz las correcciones necesarias.
            
            --- OBSERVACIONES DEL USUARIO PARA REFINAR ---
            {feedback_usuario}
            -------------------------------------------
            
            --- √çTEM ORIGINAL A REFINAR ---
            {item_a_refinar_text}
            -----------------------------
            
            --- INSTRUCCIONES DE SALIDA ---
            Devuelve el √≠tem refinado con el mismo formato original:
            PREGUNTA: ...
            A. ...
            B. ...
            C. ...
            D. ...
            RESPUESTA CORRECTA: ...
            JUSTIFICACIONES:
            A. ...
            B. ...
            C. ...
            D. ...
            GRAFICO_NECESARIO: [S√ç/NO]
            DESCRIPCION_GRAFICO: [Descripci√≥n detallada o N/A]
            """
            
            try:
                # Quitamos el spinner detallado de aqu√≠
                full_llm_response = generar_texto_con_llm(gen_model_name, prompt_refinamiento)
            
                if full_llm_response is None:
                    st.error("Fallo en la refinaci√≥n del √≠tem.")
                    return None
            
                item_and_graphic_match = re.search(r"(PREGUNTA:.*?)(GRAFICO_NECESARIO:\s*(S√ç|NO).*?DESCRIPCION_GRAFICO:.*)", full_llm_response, re.DOTALL)
                if item_and_graphic_match:
                    current_item_text = item_and_graphic_match.group(1).strip()
                    grafico_info_block = item_and_graphic_match.group(2).strip()
                    grafico_necesario_match = re.search(r"GRAFICO_NECESARIO:\s*(S√ç|NO)", grafico_info_block)
                    if grafico_necesario_match:
                        grafico_necesario = grafico_necesario_match.group(1).strip()
                    descripcion_grafico_match = re.search(r"DESCRIPCION_GRAFICO:\s*(.*)", grafico_info_block, re.DOTALL)
                    if descripcion_grafico_match:
                        # --- INICIO DEL NUEVO BLOQUE DE C√ìDIGO ---
                        descripcion_grafico_str = descripcion_grafico_match.group(1).strip()
                        descripciones_graficos_list = [] # Nueva variable para la lista

                        if descripcion_grafico_str.upper() != 'N/A' and descripcion_grafico_str.strip().startswith('['):
                            try:
                                descripciones_graficos_list = json.loads(descripcion_grafico_str)
                            except json.JSONDecodeError:
                                print("Error al decodificar JSON de gr√°ficos (generaci√≥n). Tratando como texto simple.")
                                descripciones_graficos_list = [{"ubicacion": "enunciado", "tipo_elemento": "otro_tipo", "datos": {"descripcion_natural": descripcion_grafico_str}}]
                        elif descripcion_grafico_str.upper() != 'N/A':
                             # Si no es una lista, lo tratamos como un solo gr√°fico para enunciado
                            descripciones_graficos_list = [{"ubicacion": "enunciado", "tipo_elemento": "otro_tipo", "datos": {"descripcion_natural": descripcion_grafico_str}}]

                        # Asignamos la lista a la variable original para que el resto del c√≥digo la use
                        descripcion_grafico = descripciones_graficos_list
                        # --- FIN DEL NUEVO BLOQUE DE C√ìDIGO ---
                else:
                    current_item_text = full_llm_response
                    grafico_necesario = "NO"
                    descripcion_grafico = ""
            
                auditoria_resultado, full_auditor_prompt = auditar_item_con_llm(
                    audit_model_name,
                    item_generado=current_item_text,
                    grado=grado_elegido, area=area_elegida, asignatura=asignatura_elegida, macrohabilidad=macrohabilidad_elegida,
                    proceso_cognitivo=proceso_cognitivo_elegido,
                    microhabilidad=microhabilidad_elegida, competencia_microhabilidad=competencia_microhabilidad_elegida,
                    contexto_educativo=contexto_educativo, manual_reglas_texto=manual_reglas_texto,
                    descripcion_bloom=descripcion_bloom,
                    grafico_necesario=grafico_necesario,
                    descripcion_grafico=descripcion_grafico,
                    prompt_auditor_adicional=prompt_auditor_adicional
                )
    
                dictamen_final_match = re.search(r"DICTAMEN FINAL:\s*\[(.*?)]", auditoria_resultado, re.DOTALL)
                auditoria_status = dictamen_final_match.group(1).strip() if dictamen_final_match else "‚ùå RECHAZADO (no se pudo extraer dictamen)"
                observaciones_start = auditoria_resultado.find("OBSERVACIONES FINALES:")
                audit_observations = auditoria_resultado[observaciones_start + len("OBSERVACIONES FINALES:"):].strip() if observaciones_start != -1 else "No se pudieron extraer observaciones espec√≠ficas."
                
                item_final_data = {
                    "item_text": current_item_text,
                    "classification": classification_details,
                    "grafico_necesario": grafico_necesario,
                    "descripciones_graficos": descripcion_grafico,
                    "final_audit_status": auditoria_status,
                    "final_audit_observations": audit_observations,
                    "generation_prompt_used": prompt_refinamiento,
                    "auditor_prompt_used": full_auditor_prompt
                }
                return item_final_data
            
            except Exception as e:
                audit_observations = f"Error t√©cnico durante la refinaci√≥n: {e}. Por favor, corrige este problema."
                auditoria_status = "‚ùå RECHAZADO (error t√©cnico)"
                item_final_data = {
                    "item_text": current_item_text if current_item_text else "No se pudo refinar el √≠tem debido a un error t√©cnico.",
                    "classification": classification_details,
                    "grafico_necesario": "NO",
                    "descripcion_grafico": "",
                    "descripciones_graficos": "",
                    "final_audit_status": auditoria_status,
                    "final_audit_observations": audit_observations,
                    "generation_prompt_used": prompt_refinamiento,
                    "auditor_prompt_used": full_auditor_prompt
                }
                return item_final_data
        else:
            while auditoria_status != "‚úÖ CUMPLE TOTALMENTE" and attempt < max_refinement_attempts:
                attempt += 1
                
                # --- INICIO DE LA NUEVA L√ìGICA REFORZADA PARA EL CONTEXTO ---
                instruccion_contexto = ""
                formato_salida_pregunta = "PREGUNTA: [Redacta aqu√≠ el contexto (si es necesario) y el enunciado de la pregunta]"
    
                if contexto_general_macrohabilidad:
                    # Si S√ç hay un contexto compartido, las instrucciones son estrictas.
                    instruccion_contexto = f"""
    --- CONTEXTO GENERAL OBLIGATORIO DE LA MACROHABILIDAD ---
    ¬°INSTRUCCI√ìN CR√çTICA! Debes iniciar el campo 'PREGUNTA:' exactamente con el siguiente texto de contexto, sin alterarlo, resumirlo o parafrasearlo. Despu√©s del contexto, redacta el enunciado espec√≠fico para el √≠tem.
    
    CONTEXTO GENERAL DE LA MACROHABILIDAD (DEBE SER INCLUIDO TEXTUALMENTE):
    "{contexto_general_macrohabilidad}"
    ----------------------------------------------------
    """
                    formato_salida_pregunta = "PREGUNTA: [Texto del CONTEXTO GENERAL DE LA MACROHABILIDAD] [Enunciado espec√≠fico de la pregunta]"
                else:
                    # Si NO hay contexto, las instrucciones son las normales.
                    instruccion_contexto = """
    --- CONTEXTO GENERAL DE LA MACROHABILIDAD (si aplica) ---
    Este √≠tem debe generar su propio contexto individual, ya que no se ha definido un contexto general para la macrohabilidad.
    ----------------------------------------------------
    """
                # --- FIN DE LA NUEVA L√ìGICA ---
    
                clave_aleatoria = random.choice(['A', 'B', 'C', 'D'])

                seccion_contexto_libro = ""
                if contexto_del_libro:
                    seccion_contexto_libro = f"""
        --- CONTEXTO PRINCIPAL DEL LIBRO GU√çA (¬°USO OBLIGATORIO!) ---
        ¬°INSTRUCCI√ìN CR√çTICA! Debes basar tu pregunta, opciones y justificaciones **principalmente** en los siguientes extractos del libro gu√≠a. La respuesta correcta DEBE poder deducirse de este texto.
        
        {contexto_del_libro}
        ---------------------------------------------------------
        """
                #
                seccion_imagen = ""
                if descripcion_imagen_aprobada:
                    seccion_imagen = f"""
    --- INFORMACI√ìN VISUAL OBLIGATORIA (BASADA EN IMAGEN) ---
    ¬°INSTRUCCI√ìN CR√çTICA! El √≠tem que construyas DEBE basarse directamente en la siguiente descripci√≥n de una imagen. La pregunta, las opciones y las justificaciones deben hacer referencia a los detalles mencionados aqu√≠. Este es el insumo principal.
    
    DESCRIPCI√ìN DE LA IMAGEN:
    "{descripcion_imagen_aprobada}"
    ---------------------------------------------------------
    """
    
                prompt_content_for_llm = f"""
                Eres un psic√≥metra y dise√±ador experto en √≠tems de evaluaci√≥n educativa, con profundo conocimiento en la Taxonom√≠a de Bloom y su aplicaci√≥n pr√°ctica.
                Tu tarea es construir un √≠tem de {tipo_pregunta} con una √∫nica respuesta correcta, garantizando una alineaci√≥n perfecta y demostrable con el marco cognitivo solicitado, siguiendo un riguroso proceso de an√°lisis previo.
                
                --- CONTEXTO Y PAR√ÅMETROS DEL √çTEM ---
                - Grado: {grado_elegido}
                - √Årea: {area_elegida}
                - Asignatura: {asignatura_elegida}
                - Macrohabilidad o unidad tem√°tica: {macrohabilidad_elegida}
                - Proceso cognitivo (Taxonom√≠a de Bloom): {proceso_cognitivo_elegido}
                - Descripci√≥n DETALLADA y VINCULANTE del proceso cognitivo:
                    "{descripcion_bloom}"

                --- EJEMPLOS Y GU√çAS DE PREGUNTAS (Preguntas Tipo) ---
                ¬°INSTRUCCI√ìN CLAVE! Para asegurar que el enunciado del √≠tem se alinee con el proceso cognitivo, insp√≠rate en los siguientes ejemplos. La pregunta que formules debe seguir un estilo similar, buscando una √∫nica respuesta correcta y evitando comparaciones subjetivas ("mejor", "m√°s adecuado").
                {ejemplos_preguntas}
                ----------------------------------------------------
                
                --- PROMPT ADICIONAL: TAXONOM√çA DE BLOOM / PROCESOS COGNITIVOS ---
                {prompt_bloom_adicional if prompt_bloom_adicional else "No se proporcionaron prompts adicionales espec√≠ficos para taxonom√≠a de Bloom."}
                ------------------------------------------------------------------
                
                - Microhabilidad (foco principal del √≠tem): {microhabilidad_elegida}
                - Nivel educativo esperado del estudiante: {contexto_educativo}
                - Nivel de dificultad deseado: {dificultad}
                
                {instruccion_contexto}

                {seccion_imagen}

                {seccion_contexto_libro}

                # =============================================================================
                # INICIO DE LA MODIFICACI√ìN CLAVE: AN√ÅLISIS COGNITIVO OBLIGATORIO Y EXCLUSIVO
                # =============================================================================
                --- AN√ÅLISIS COGNITIVO OBLIGATORIO (TAXONOM√çA DE BLOOM) ---
                Antes de escribir el √≠tem, DEBES realizar el siguiente an√°lisis interno para garantizar una alineaci√≥n perfecta. La calidad de tu pregunta depender√° de la rigurosidad de este an√°lisis.
                
                1.  **Deconstrucci√≥n del Proceso Cognitivo**: Revisa la "Descripci√≥n DETALLADA y VINCULANTE del proceso cognitivo" proporcionada. Es de car√°cter **obligatorio** que extraigas de ella el subproceso y los **verbos de acci√≥n clave** o sin√≥nimos directos que mejor se alineen con la microhabilidad '{microhabilidad_elegida}'.
                
                2.  **Dise√±o de la Tarea Cognitiva**: Describe la tarea mental espec√≠fica y observable que el estudiante DEBE realizar. **Esta descripci√≥n debe incorporar expl√≠citamente los verbos de acci√≥n (o sus sin√≥nimos directos) que identificaste en el paso anterior.** No describas la pregunta, sino la operaci√≥n mental. (Ej: "La tarea exige que el estudiante *compare* dos eventos hist√≥ricos para *detectar correspondencias* entre sus causas econ√≥micas, y luego *construya un modelo* simple de causa-efecto que *explique* esas similitudes.").
                
                3.  **Justificaci√≥n de la Alineaci√≥n**: Justifica expl√≠citamente c√≥mo la "Tarea Cognitiva" que dise√±aste se alinea con la definici√≥n del proceso "{proceso_cognitivo_elegido}" y su subproceso. (Ej: "Esta tarea se alinea con COMPRENDER-Comparar y Explicar porque el estudiante debe procesar informaci√≥n, detectar relaciones y construir un modelo causal, lo cual va m√°s all√° de solo recordar los hechos.").
                
                4.  **Verificaci√≥n de Exclusividad Cognitiva (¬°CR√çTICO!)**: Debes confirmar que la tarea dise√±ada NO pertenece a otros niveles cognitivos. Justifica brevemente por qu√© la tarea:
                    * **Supera el nivel anterior**: Explica por qu√© la tarea es m√°s compleja que el nivel cognitivo inmediatamente inferior en la taxonom√≠a. (Ej: "No es solo RECORDAR porque no se pide evocar fechas, sino relacionarlas.").
                    * **No alcanza el nivel superior**: Explica por qu√© la tarea no llega a la complejidad del nivel cognitivo inmediatamente superior. (Ej: "No es ANALIZAR porque no se le pide que deconstruya la validez de las fuentes de informaci√≥n o que determine sesgos, solo que organice y explique la informaci√≥n presentada.").
                
                La pregunta que construir√°s a continuaci√≥n debe ser la materializaci√≥n exacta de esta Tarea Cognitiva verificada.
                # =============================================================================
                # FIN DE LA MODIFICACI√ìN CLAVE
                # =============================================================================
                
                --- INSTRUCCIONES PARA LA CONSTRUCCI√ìN DEL √çTEM ---
                CONTEXTO DEL √çTEM:
                - Debe ser relevante y plausible, sirviendo como el escenario donde se ejecutar√° la Tarea Cognitiva que dise√±aste.
                - La tem√°tica debe ser la de la {macrohabilidad_elegida} y ser central para el problema.
                - Evita referencias a marcas, nombres propios, lugares reales o informaci√≥n personal identificable.
                
                ENUNCIADO:
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - **CR√çTICO**: Formula una pregunta que fuerce al estudiante a ejecutar la Tarea Cognitiva que definiste y verificaste en tu an√°lisis. El enunciado es el disparador de esa operaci√≥n mental.
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - Formula una pregunta clara, directa, sin ambig√ºedades ni tecnicismos innecesarios.
                - ¬°INSTRUCCI√ìN CR√çTICA DE ESTILO! Evita terminantemente formular preguntas que pidan al estudiante comparar o jerarquizar opciones. **NO USES** frases como "¬øcu√°l es la opci√≥n m√°s...", "¬øcu√°l es el mejor...", "¬øcu√°l describe principalmente...?", "¬øcu√°l es la raz√≥n principal...?". La pregunta debe tener una √∫nica respuesta objetivamente correcta.
                - En su lugar, formula preguntas directas como: "**¬øCu√°l es la causa de...?**", "**¬øQu√© conclusi√≥n se deriva de...?**", "**¬øCu√°l de las afirmaciones es correcta?**".
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - NO uses directamente en la pregunta el verbo principal del proceso cognitivo (ej. no preguntes "¬øCu√°l es el an√°lisis de...?"). Busca redacciones m√°s aut√©nticas.
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - Si utilizas negaciones, res√°ltalas en MAY√öSCULAS Y NEGRITA (por ejemplo: **NO ES**, **EXCEPTO**).
                
                OPCIONES DE RESPUESTA:
                - Escribe exactamente cuatro opciones (A, B, C y D).
                - **Opci√≥n Correcta**: Debe ser la √∫nica conclusi√≥n v√°lida tras ejecutar correctamente la Tarea Cognitiva.
                - La respuesta correcta DEBE ser la opci√≥n {clave_aleatoria}.
                - **Distractores (Incorrectos)**: Deben ser plausibles y dise√±ados a partir de errores t√≠picos en la ejecuci√≥n de la Tarea Cognitiva. (Ej: un distractor podr√≠a ser el resultado de aplicar un proceso cognitivo inferior, como simplemente recordar un dato, en lugar de analizarlo).
                - Las respuestas deben tener una estructura gramatical y longitud similares.
                - No utilices f√≥rmulas vagas como ‚Äúninguna de las anteriores‚Äù o ‚Äútodas las anteriores‚Äù.
                
                JUSTIFICACIONES:
                {formato_justificacion}
    
                --- PROMPT ADICIONAL: REGLAS GENERALES DE CONSTRUCCI√ìN ---
                {prompt_construccion_adicional if prompt_construccion_adicional else "No se proporcionaron prompts adicionales espec√≠ficos para reglas generales de construcci√≥n."}
                ---------------------------------------------------------
    
                --- REGLAS ADICIONALES DEL MANUAL DE CONSTRUCCI√ìN ---
                Considera y aplica estrictamente todas las directrices, ejemplos y restricciones contenidas en el siguiente manual.
                Esto es de suma importancia para la calidad y pertinencia del √≠tem.
    
                Manual de Reglas:
                {manual_reglas_texto}
                ----------------------------------------------------
    
                --- INFORMACI√ìN ADICIONAL PROPORCIONADA POR EL USUARIO (Contexto General) ---
                {informacion_adicional_usuario if informacion_adicional_usuario else "No se proporcion√≥ informaci√≥n adicional general."}
                ---------------------------------------------------------------------------
                
                --- PROMPT ADICIONAL: COSAS ESPEC√çFICAS A TENER EN CUENTA ---
                {prompt_especifico_adicional if prompt_especifico_adicional else "No se proporcionaron prompts adicionales espec√≠ficos para consideraciones adicionales."}
                ----------------------------------------------------------
    
                --- DATO CLAVE PARA LA CONSTRUCCI√ìN ---
                Basado en el foco tem√°tico y el proceso cognitivo, considera el siguiente dato o idea esencial:
                "{dato_para_pregunta_foco}"
    
                --- INSTRUCCIONES ESPEC√çFICAS DE SALIDA PARA GR√ÅFICO ---
                Despu√©s del bloque de JUSTIFICACIONES, indica si el √≠tem necesita elementos visuales.
                
                ¬°INSTRUCCI√ìN CR√çTICA! **Considera como elemento visual cualquier cosa que no sea texto de prosa**, incluyendo: gr√°ficos, diagramas, **tablas**, construcciones geom√©tricas, etc.
                
                GRAFICO_NECESARIO: [S√ç/NO]
                DESCRIPCION_GRAFICO: [Si la respuesta es **NO**, escribe **N/A**. Si la respuesta es **S√ç**, DEBES proporcionar una **LISTA DE OBJETOS JSON V√ÅLIDOS** incluso si solo hay un gr√°fico, siguiendo estas reglas:]
                El JSON siempre debe contener los campos: `"ubicacion"`, `"tipo_elemento"`, `"datos"`, `"configuracion"` y `"descripcion"`.

                1. Cada objeto DEBE contener una clave `"ubicacion"` para identificar d√≥nde va el gr√°fico. Usa uno de los siguientes valores: `"enunciado"`, `"opcion_a"`, `"opcion_b"`, `"opcion_c"`, `"opcion_d"`.
                
                2.  Para `"tipo_elemento"`, elige **UNO** de la siguiente lista: `grafico_barras_verticales`, `grafico_circular`, `tabla`, `construccion_geometrica`, `diagrama_arbol`, `flujograma`, `pictograma`, `scatter_plot`, `line_plot`, `histogram`, `box_plot`, `violin_plot`, `heatmap`, `contour_plot`, `3d_plot`, `network_diagram`, `area_plot`, `radar_chart`, `venn_diagram`, `fractal`, `otro_tipo`.
                
                3.  Para `"descripcion"`, proporciona un **texto en lenguaje natural que resuma y detalle todos los elementos clave del gr√°fico**, sus relaciones y las caracter√≠sticas que se deben tener en cuenta para generarlo visualmente.
                
                4.  **L√ìGICA CONDICIONAL PARA EL CAMPO "datos":**
                    * **Si eliges un `tipo_elemento` de la lista (QUE NO SEA `otro_tipo`)**: El campo `"datos"` debe ser un objeto con la **informaci√≥n estructurada y num√©rica**.
                        * *Ejemplo para `tabla`*:
                        ```json
                        {{
                          "ubicacion": "enunciado",
                          "tipo_elemento": "tabla",
                          "datos": {{
                            "columnas": ["Pa√≠s", "Capital"],
                            "filas": [["Colombia", "Bogot√°"], ["Argentina", "Buenos Aires"]]
                          }},
                          "configuracion": {{ "titulo": "Capitales de Sudam√©rica" }},
                          "descripcion": "Una tabla de dos columnas que lista pa√≠ses sudamericanos y sus respectivas capitales. La primera columna corresponde al pa√≠s y la segunda a su capital."
                        }}
                        ```
                    * **Si el gr√°fico no corresponde a ninguno y eliges `otro_tipo`**: El campo `"datos"` debe contener un √∫nico objeto con la clave `"descripcion_natural"`, cuyo valor ser√° un **texto exhaustivo** con todos los detalles necesarios para construir el gr√°fico desde cero.
                        * *Ejemplo para `otro_tipo`*:
                        ```json
                        {{
                          "ubicacion": "opcion_a",
                          "tipo_elemento": "otro_tipo",
                          "datos": {{
                            "descripcion_natural": "Se requiere un diagrama de un circuito el√©ctrico simple en serie. Debe mostrar una fuente de poder (bater√≠a) de 9V conectada a tres resistencias (R1=10Œ©, R2=20Œ©, R3=30Œ©) una despu√©s de la otra. El diagrama debe indicar claramente la direcci√≥n del flujo de la corriente (I) con una flecha saliendo del polo positivo de la bater√≠a."
                          }},
                          "configuracion": {{ "titulo": "Circuito en Serie" }},
                          "descripcion": "Diagrama de un circuito el√©ctrico simple con una bater√≠a y tres resistencias conectadas en serie, mostrando el flujo de la corriente."
                        }}
                        ```
 
                --- FORMATO ESPERADO DE SALIDA ---
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬°INSTRUCCI√ìN CR√çTICA! Tu respuesta DEBE ser un √∫nico bloque de c√≥digo JSON v√°lido, sin ning√∫n otro texto o explicaci√≥n antes o despu√©s (no uses \`\`\`json).
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† El objeto JSON debe tener la siguiente estructura:              
                {{
                  "pregunta": "Aqu√≠ va el texto del contexto (si lo hay) seguido del enunciado de la pregunta.",
                  "opciones": {{
                    "A": "Texto de la opci√≥n A.",
                    "B": "Texto de la opci√≥n B.",
                    "C": "Texto de la opci√≥n C.",
                    "D": "Texto de la opci√≥n D."
                  }},
                  "respuestaCorrecta": "{clave_aleatoria}",
                  "justificaciones": {{
                    "A": "Justificaci√≥n para la opci√≥n A.",
                    "B": "Justificaci√≥n para la opci√≥n B.",
                    "C": "Justificaci√≥n para la opci√≥n C.",
                    "D": "Justificaci√≥n para la opci√≥n D."
                  }},
                  "graficoNecesario": "S√ç",
                  "descripcionGrafico": [
                    {{
                      "ubicacion": "enunciado",
                      "tipo_elemento": "tabla",
                      "datos": {{"columnas": ["X"], "filas": [[1]]}},
                      "configuracion": {{"titulo": "Ejemplo"}},
                      "descripcion": "Descripci√≥n del gr√°fico."
                    }}
                  ]
                }}

¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Aseg√∫rate de que el valor de "respuestaCorrecta" sea exactamente "{clave_aleatoria}". Si "graficoNecesario" es "NO", el valor de "descripcionGrafico" debe ser un array vac√≠o [].
                """
                
                if attempt > 1:
                    prompt_content_for_llm += f"""
                    --- RETROALIMENTACI√ìN DE AUDITOR√çA PARA REFINAMIENTO ---
                    El √≠tem anterior no cumpli√≥ con todos los criterios. Por favor, revisa las siguientes observaciones y mejora el √≠tem para abordarlas.
                    Observaciones del Auditor:
                    {audit_observations}
                    ---------------------------------------------------
                    --- √çTEM ANTERIOR A REFINAR ---
                    {current_item_text}
                    -------------------------------
                    """
                
                full_generation_prompt = prompt_content_for_llm
    
                try:
                    # Quitamos el spinner de aqu√≠
                    full_llm_response = generar_texto_con_llm(gen_model_name, prompt_content_for_llm)
                    
                    if full_llm_response is None:
                        auditoria_status = "‚ùå RECHAZADO (Error de Generaci√≥n)"
                        audit_observations = "El modelo de generaci√≥n no pudo producir una respuesta v√°lida."
                        break
                    
                    # ... (dentro del while y el primer try/except) ...
                    # -- INICIO DEL NUEVO BLOQUE DE PARSEO JSON --
                    try:
                        # --- INICIO DE LA MODIFICACI√ìN ---
                        # 1. Busca el inicio del JSON (el primer '{') y el final (el √∫ltimo '}')
                        json_start = full_llm_response.find('{')
                        json_end = full_llm_response.rfind('}')

                        # 2. Verifica si se encontr√≥ un objeto JSON en la respuesta
                        if json_start != -1 and json_end != -1:
                            # 3. Extrae √∫nicamente el string del JSON
                            json_string = full_llm_response[json_start:json_end + 1]
                            # 4. Intenta decodificar S√ìLO el string extra√≠do
                            item_data = json.loads(json_string)
                        else:
                            # Si no se encontr√≥, lanza un error para que sea manejado por el bloque 'except'
                            raise json.JSONDecodeError("No se encontr√≥ un objeto JSON en la respuesta del modelo.", full_llm_response, 0)
                        # --- FIN DE LA MODIFICACI√ìN ---

                        # 2. Reconstruimos el texto del √≠tem para mostrarlo en la UI y para el auditor
                        opciones_texto = "\n".join([f"{key}. {value}" for key, value in item_data.get("opciones", {}).items()])
                        justificaciones_texto = "\n".join([f"{key}. {value}" for key, value in item_data.get("justificaciones", {}).items()])
                        
                        current_item_text = (
                            f"PREGUNTA: {item_data.get('pregunta', '')}\n"
                            f"{opciones_texto}\n"
                            f"RESPUESTA CORRECTA: {item_data.get('respuestaCorrecta', '')}\n"
                            f"JUSTIFICACIONES:\n{justificaciones_texto}"
                        )
                        
                        # 3. Extraemos la informaci√≥n del gr√°fico directamente
                        grafico_necesario = item_data.get("graficoNecesario", "NO")
                        # El prompt ya pide que sea una lista de objetos, as√≠ que la obtenemos directamente
                        descripciones_graficos_list = item_data.get("descripcionGrafico", [])
                        descripcion_grafico = descripciones_graficos_list # Asignamos para la auditor√≠a


                    except json.JSONDecodeError:
                        # Si el LLM no devuelve un JSON v√°lido, lo marcamos como un error de formato
                        auditoria_status = "‚ùå RECHAZADO (Error de Formato JSON)"
                        audit_observations = f"El modelo de generaci√≥n no produjo un JSON v√°lido. Salida recibida:\n{full_llm_response}"
                        st.warning(audit_observations)
                        current_item_text = full_llm_response # Guardamos el texto err√≥neo para el reintento
                        grafico_necesario = "NO"
                        descripcion_grafico = ""
                        # Forzamos la salida del bucle de reintentos si hay error de formato
                    # -- FIN DEL NUEVO BLOQUE DE PARSEO JSON --
                    
                    # Quitamos el spinner de aqu√≠

                    auditoria_json_str, full_auditor_prompt = auditar_item_con_llm(
                        audit_model_name,
                        item_generado=current_item_text,
                        grado=grado_elegido, area=area_elegida, asignatura=asignatura_elegida, macrohabilidad=macrohabilidad_elegida,
                        proceso_cognitivo=proceso_cognitivo_elegido,
                        microhabilidad=microhabilidad_elegida, competencia_microhabilidad=competencia_microhabilidad_elegida,
                        contexto_educativo=contexto_educativo, manual_reglas_texto=manual_reglas_texto,
                        descripcion_bloom=descripcion_bloom,
                        grafico_necesario=grafico_necesario,
                        descripcion_grafico=descripcion_grafico,
                        prompt_auditor_adicional=prompt_auditor_adicional
                    )
    
                    if auditoria_json_str is None:
                        auditoria_status = "‚ùå RECHAZADO (Error de Auditor√≠a)"
                        audit_observations = "El modelo de auditor√≠a no pudo producir una respuesta v√°lida."
                    else:
                        auditoria_data = parse_json_llm(auditoria_json_str)
                        if auditoria_data is None:
                            auditoria_status = "‚ùå RECHAZADO (Error de Formato JSON del Auditor)"
                            audit_observations = f"El modelo de auditor√≠a no produjo un JSON v√°lido. Salida recibida:\n{auditoria_json_str}"
                            st.warning(audit_observations)
                        else:
                            auditoria_status = auditoria_data.get("dictamen_final", "‚ùå RECHAZADO (Clave no encontrada)")
                            audit_observations = auditoria_data.get("observaciones_finales", "No se encontraron observaciones.")
                    
                    item_final_data = {
                        "item_text": current_item_text,
                        "classification": classification_details,
                        "grafico_necesario": grafico_necesario,
                        "descripciones_graficos": descripciones_graficos_list,
                        "final_audit_status": auditoria_status,
                        "final_audit_observations": audit_observations,
                        "generation_prompt_used": full_generation_prompt,
                        "auditor_prompt_used": full_auditor_prompt
                    }
        
                    if auditoria_status == "‚úÖ CUMPLE TOTALMENTE":
                        break

                
                except Exception as e:
                    audit_observations = f"Error t√©cnico durante la generaci√≥n: {e}. Por favor, corrige este problema."
                    auditoria_status = "‚ùå RECHAZADO (error t√©cnico)"
                    item_final_data = {
                        "item_text": current_item_text if current_item_text else "No se pudo generar el √≠tem debido a un error t√©cnico.",
                        "classification": classification_details,
                        "grafico_necesario": "NO",
                        "descripcion_grafico": "",
                        "final_audit_status": auditoria_status,
                        "final_audit_observations": audit_observations,
                        "generation_prompt_used": full_generation_prompt,
                        "auditor_prompt_used": full_auditor_prompt
                    }
                    break
    
            if item_final_data is None:
                return None
    
            return item_final_data

    def crear_documento_word_individual(item_data):
        """
        Crea un documento de Word en memoria para un √∫nico √≠tem procesado.
        """
        doc = docx.Document()
    
        # Extraer datos del √≠tem
        pregunta_texto = item_data.get("item_text", "No disponible")
        classification = item_data.get("classification", {})
    
        # --- A√ëADIR N√öMERO DE FILA DESDE LA COLUMNA "Numero" ---
        numero_de_fila = classification.get("Numero", "N/A")
        p_fila = doc.add_paragraph()
        run_fila_label = p_fila.add_run("Fila: ")
        run_fila_label.bold = True
        p_fila.add_run(str(numero_de_fila))
        doc.add_paragraph('')  # Espacio despu√©s
    
        # A√±adir clasificaci√≥n
        doc.add_paragraph('--- Clasificaci√≥n del √çtem ---')
        for key, value in classification.items():
            p = doc.add_paragraph()
            run = p.add_run(f"{key}: ")
            run.bold = True
            p.add_run(str(value))
    
        doc.add_paragraph('')
    
        # --- INICIO DE LA MODIFICACI√ìN ---
        # A√±adir la imagen de origen si existe
        if 'source_image' in item_data and item_data['source_image']:
            doc.add_heading('Insumo Visual de Origen', level=2)
            try:
                # Regresamos al inicio del buffer de la imagen
                item_data['source_image'].seek(0)
                doc.add_picture(item_data['source_image'], width=Inches(5.0))
            except Exception as e:
                doc.add_paragraph(f"No se pudo incrustar la imagen de origen. Error: {e}")
            doc.add_paragraph('')
        # --- FIN DE LA MODIFICACI√ìN ---

        # A√±adir texto del √≠tem (pregunta, opciones, etc.)
        doc.add_paragraph('--- Contenido del √çtem ---')
        doc.add_paragraph(pregunta_texto)
        doc.add_paragraph('')
    
        # --- Obtener info de gr√°fico del item_data (evita NameError) ---
        grafico_necesario_val = str(item_data.get("grafico_necesario", "NO") or "NO")
        desc_grafico = item_data.get("descripcion_grafico", "") or ""
    
        # --- Gr√°fico (solo si es requerido) ---
        def _es_si(s):
            s = str(s).lower().strip().replace("√≠", "i")  # normaliza "s√≠" -> "si"
            return s == "si"
               
        # --- INICIO DEL BLOQUE DE ANEXO FINAL ---
        grafico_necesario_val = str(item_data.get("grafico_necesario", "NO") or "NO").lower().strip()
    
        if grafico_necesario_val == 's√≠' or grafico_necesario_val == 'si':
            descripciones_graficos = item_data.get("descripciones_graficos", [])
            imagenes_guardadas = item_data.get("generated_images", [])
            
            if descripciones_graficos:
                doc.add_page_break()
                doc.add_heading('Anexo de Gr√°ficos', level=1)
                
                for i, desc_grafico in enumerate(descripciones_graficos):
                    ubicacion = desc_grafico.get("ubicacion", f"gr√°fico_{i+1}")
                    ubicacion_titulo = ubicacion.replace("_", " ").title()
                    
                    doc.add_heading(f"Gr√°fico {i+1}: Para {ubicacion_titulo}", level=2)
                    
                    # Buscamos la imagen correspondiente a esta descripci√≥n
                    imagen_encontrada = None
                    for img in imagenes_guardadas:
                        if img.get("ubicacion") == ubicacion:
                            imagen_encontrada = img.get("buffer")
                            break
                    
                    if imagen_encontrada:
                        # ¬°Incrustamos la imagen!
                        imagen_encontrada.seek(0) # Regresamos al inicio del buffer de la imagen
                        doc.add_picture(imagen_encontrada, width=Inches(5.5))
                    else:
                        # Si no hay imagen, guardamos la descripci√≥n como respaldo
                        doc.add_paragraph("No se gener√≥ una imagen para este gr√°fico. Se adjunta su descripci√≥n t√©cnica:")
                        json_str = json.dumps(desc_grafico, indent=2, ensure_ascii=False)
                        p = doc.add_paragraph(json_str)
                        p.style = 'Quote'
        # --- FIN DEL BLOQUE DE ANEXO FINAL ---
    
        # Guardar en un buffer de memoria y devolverlo
        buffer = io.BytesIO()
        doc.save(buffer)
        buffer.seek(0)
        return buffer

    def exportar_a_zip(preguntas_procesadas_list):
        """
        Crea un archivo .zip en memoria que contiene un .docx individual para cada √≠tem aprobado.
        Nombra cada .docx seg√∫n su ID y maneja duplicados con sufijos (_A, _B, C...).
        """
        zip_buffer = io.BytesIO()
        id_counts = {}

        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            for item_data in preguntas_procesadas_list:
                item_id = item_data.get("classification", {}).get("ID", "sin_id")
                
                # Contar cu√°ntas veces hemos usado este ID
                count = id_counts.get(item_id, 0)
                
                # Generar el nombre de archivo √∫nico
                if count > 0:
                    # A√±adir sufijo _B, _C, etc. (chr(65) es 'A')
                    sufijo = f"_{chr(65 + count)}"
                    nombre_archivo = f"{item_id}{sufijo}.docx"
                else:
                    # Para la primera aparici√≥n, podr√≠a ser _A o sin sufijo. Usemos _A por consistencia.
                    nombre_archivo = f"{item_id}_A.docx"

                id_counts[item_id] = count + 1

                # Crear el documento Word individual en memoria
                doc_buffer = crear_documento_word_individual(item_data)
                
                # A√±adir el archivo Word al ZIP
                zip_file.writestr(nombre_archivo, doc_buffer.getvalue())

        zip_buffer.seek(0)
        return zip_buffer
        
    # --- NUEVA FUNCI√ìN: Generar imagen con IA (Indentaci√≥n y robustez corregidas) ---
    def generar_imagen_con_ia(prompt_descripcion: str):
        """
        Genera una imagen desde una descripci√≥n textual usando Vertex AI.
        Devuelve un BytesIO con la imagen o None si falla.
        """
        if not prompt_descripcion or not str(prompt_descripcion).strip():
            st.warning("El prompt de la imagen est√° vac√≠o.")
            return None
    
        try:
            # Aseg√∫rate de haber llamado vertexai.init(project=..., location=...) antes en tu app.
            modelo_imagen = ImageGenerationModel.from_pretrained("imagen-4.0-fast-generate-001")
    
            respuesta = modelo_imagen.generate_images(
                prompt=prompt_descripcion,
                number_of_images=1,
                # Ejemplos de par√°metros opcionales:
                # aspect_ratio="1:1",
                # negative_prompt="texto borroso, baja resoluci√≥n",
            )
    
            if not respuesta or not getattr(respuesta, "images", None):
                raise ValueError("El modelo no devolvi√≥ im√°genes.")
    
            img_obj = respuesta.images[0]
    
            # Preferimos atributo p√∫blico si existe; si no, fallback al interno (_image_bytes)
            image_bytes = getattr(img_obj, "image_bytes", None) or getattr(img_obj, "_image_bytes", None)
    
            # Algunos SDKs exponen m√©todo para bytes
            if image_bytes is None and hasattr(img_obj, "as_bytes"):
                image_bytes = img_obj.as_bytes()
    
            if image_bytes is None:
                raise ValueError("No fue posible extraer los bytes de la imagen.")
    
            return io.BytesIO(image_bytes)
    
        except Exception as e:
            st.error(f"Error al generar imagen con IA: {e}")
            return None
    # --- FIN DE LA NUEVA FUNCI√ìN ---
         
    def exportar_a_excel(preguntas_procesadas_list, nombre_archivo_base):
        """
        Exporta los √≠tems procesados a un archivo Excel con una estructura detallada.
        """
        datos_para_excel = []
    
        for i, item_data in enumerate(preguntas_procesadas_list):
            item_text = item_data.get("item_text", "")
            classification = item_data.get("classification", {})
        
            pregunta_bloque_match = re.search(r"PREGUNTA:(.*?)(?=A\.)", item_text, re.S)
            contexto = ""
            enunciado = ""
            if pregunta_bloque_match:
                bloque_completo = pregunta_bloque_match.group(1).strip()
                ultimo_interrogante = bloque_completo.rfind('?')
                if ultimo_interrogante != -1:
                    contexto = bloque_completo[:ultimo_interrogante+1].strip()
                    enunciado = bloque_completo[ultimo_interrogante+1:].strip()
                else:
                    contexto = bloque_completo
            
            opcion_a_match = re.search(r"\nA\.\s(.*?)(?=\nB\.)", item_text, re.S)
            opcion_b_match = re.search(r"\nB\.\s(.*?)(?=\nC\.)", item_text, re.S)
            opcion_c_match = re.search(r"\nC\.\s(.*?)(?=\nD\.)", item_text, re.S)
            opcion_d_match = re.search(r"\nD\.\s(.*?)(?=\nRESPUESTA CORRECTA:)", item_text, re.S)
            
            clave_match = re.search(r"RESPUESTA CORRECTA:\s*(\w)", item_text)
            
            justificaciones_bloque_match = re.search(r"JUSTIFICACIONES:(.*)", item_text, re.S)
            just_a, just_b, just_c, just_d = "", "", "", ""
            if justificaciones_bloque_match:
                just_texto = justificaciones_bloque_match.group(1)
                just_a_match = re.search(r"A\.\s(.*?)(?=\n\s*B\.|\Z)", just_texto, re.S)
                just_b_match = re.search(r"B\.\s(.*?)(?=\n\s*C\.|\Z)", just_texto, re.S)
                just_c_match = re.search(r"C\.\s(.*?)(?=\n\s*D\.|\Z)", just_texto, re.S)
                just_d_match = re.search(r"D\.\s(.*)", just_texto, re.S)
                if just_a_match: just_a = just_a_match.group(1).strip()
                if just_b_match: just_b = just_b_match.group(1).strip()
                if just_c_match: just_c = just_c_match.group(1).strip()
                if just_d_match: just_d = just_d_match.group(1).strip()
                
            fila = {
                'item': f"{nombre_archivo_base}_{i+1}",
                'ID': classification.get('ID'),
                'GRADO': classification.get('GRADO'),
                '√ÅREA': classification.get('√ÅREA'),
                'ASIGNATURA': classification.get('ASIGNATURA'),
                'MACROHABILIDAD': classification.get('MACROHABILIDAD'),
                'PROCESO COGNITIVO': classification.get('PROCESO COGNITIVO'),
                'MICROHABILIDAD': classification.get('MICROHABILIDAD'),
                'COMPETENCIA MICROHABILIDAD': classification.get('COMPETENCIA MICROHABILIDAD'),
                'Contexto': contexto,
                'Enunciado': enunciado,
                'Opcion_A': opcion_a_match.group(1).strip() if opcion_a_match else "",
                'Opcion_B': opcion_b_match.group(1).strip() if opcion_b_match else "",
                'Opcion_C': opcion_c_match.group(1).strip() if opcion_c_match else "",
                'Opcion_D': opcion_d_match.group(1).strip() if opcion_d_match else "",
                'Clave': clave_match.group(1).strip() if clave_match else "",
                'Justificacion_A': just_a,
                'Justificacion_B': just_b,
                'Justificacion_C': just_c,
                'Justificacion_D': just_d,
            }
            datos_para_excel.append(fila)
    
        if not datos_para_excel:
            return None
    
        df = pd.DataFrame(datos_para_excel)
        
        column_order = ['item', 'ID', 'GRADO', '√ÅREA', 'ASIGNATURA', 'MACROHABILIDAD', 'PROCESO COGNITIVO', 
                        'MICROHABILIDAD', 'COMPETENCIA MICROHABILIDAD',
                        'Contexto', 'Enunciado', 'Opcion_A', 'Opcion_B', 'Opcion_C', 'Opcion_D', 'Clave',
                        'Justificacion_A', 'Justificacion_B', 'Justificacion_C', 'Justificacion_D']
        df = df[column_order]
    
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            df.to_excel(writer, index=False, sheet_name='Items')
            for column in df:
                column_length = max(df[column].astype(str).map(len).max(), len(column))
                writer.sheets['Items'].set_column(df.columns.get_loc(column), df.columns.get_loc(column), min(column_length, 50))
    
        buffer.seek(0)
        return buffer

    # --- L√ìGICA PRINCIPAL DE LA APLICACI√ìN ---
    
    # 1. Carga de datos desde Google Cloud Storage
    GCS_BUCKET_NAME = os.environ.get("GCS_BUCKET_NAME") 
    GCS_EXCEL_PATH = os.environ.get("GCS_EXCEL_PATH")
    GCS_PDF_PATH = os.environ.get("GCS_PDF_PATH")

    st.sidebar.header("Fuente de Datos (GCS)")
    
    df_datos = None
    manual_reglas_texto = ""

    if GCS_BUCKET_NAME and GCS_EXCEL_PATH:
        df_datos = leer_excel_desde_gcs(GCS_BUCKET_NAME, GCS_EXCEL_PATH)
    else:
        st.sidebar.error("Variables de entorno para GCS no configuradas.")
        st.info("La aplicaci√≥n requiere GCS_BUCKET_NAME y GCS_EXCEL_PATH para cargar los datos.")
        st.stop()

    if GCS_BUCKET_NAME and GCS_PDF_PATH:
        manual_reglas_texto = leer_pdf_desde_gcs(GCS_BUCKET_NAME, GCS_PDF_PATH)
        max_manual_length = 15000
        if len(manual_reglas_texto) > max_manual_length:
            st.sidebar.warning(f"Manual truncado a {max_manual_length} caracteres.")
            manual_reglas_texto = manual_reglas_texto[:max_manual_length]
        st.sidebar.info(f"Manual de reglas cargado ({len(manual_reglas_texto)} caracteres).")

    # --- A√ëADIR ESTE BLOQUE ---
    st.sidebar.markdown("---")
    st.sidebar.header("Recurso Opcional (Libro)")
    
    # Variable de sesi√≥n para el nombre del libro
    if 'processed_pdf_name' not in st.session_state:
        st.session_state.processed_pdf_name = None

    pdf_itinerario = st.sidebar.file_uploader("Subir PDF del Itinerario/Libro", type="pdf")
    #
    if pdf_itinerario:
            # Si se sube un nuevo libro O es un libro diferente al procesado
            if pdf_itinerario.name != st.session_state.processed_pdf_name:
                
                # 1. Primero, le decimos a Streamlit que TODO lo siguiente va en la barra lateral
                with st.sidebar:
                    # 2. AHORA llamamos al spinner normal, y aparecer√° en la barra lateral
                    with st.spinner(f"Procesando '{pdf_itinerario.name}'... Esto puede tardar unos minutos."):
                
                        # --- INICIO DE LA SECCI√ìN CORREGIDA ---
                        # Todo lo que sigue debe estar indentado a este nivel
                        # para que ocurra DENTRO del 'with st.spinner'
    
                        # 1. Borramos el √≠ndice viejo (si existe)
                        if 'pdf_index' in st.session_state:
                            del st.session_state['pdf_index']
                    
                        # 2. Procesamos el nuevo libro
                        pdf_bytes = pdf_itinerario.getvalue()
                        texto_completo = extraer_texto_pdf(pdf_bytes)
                        
                        if texto_completo:
                            # 3. Dividir (Chunking)
                            text_splitter = RecursiveCharacterTextSplitter(
                                chunk_size=1000, # 1000 caracteres por pedazo
                                chunk_overlap=100  # 100 caracteres de superposici√≥n
                            )
                            chunks = text_splitter.split_text(texto_completo)
                            
                            # 4. Vectorizar (Embedding) y almacenar
                            st.session_state['pdf_index'] = crear_indice_vectorial(chunks)
                            st.session_state.processed_pdf_name = pdf_itinerario.name
                            
                            # Mostramos el √©xito (a√∫n dentro del 'with st.sidebar')
                            st.sidebar.success(f"Libro '{pdf_itinerario.name}' procesado. {len(chunks)} secciones indexadas.")
                        else:
                            st.sidebar.error("El PDF est√° vac√≠o o no se pudo leer.")
                        # --- FIN DE LA SECCI√ìN CORREGIDA ---
    
            # Si el libro es el mismo que ya est√° cargado, no hacemos nada
            # y solo mostramos el mensaje de √©xito.
            elif 'pdf_index' in st.session_state:
                 st.sidebar.success(f"Libro '{pdf_itinerario.name}' listo.")
                 
    # 2. L√≥gica de Generaci√≥n y Auditor√≠a de √çtems
    st.header("Generaci√≥n y Auditor√≠a de √çtems.")
    
    if df_datos is None:
        st.error("No se pudo cargar el archivo Excel desde GCS. Verifica la configuraci√≥n.")
    else:
        # --- INTERFAZ DE USUARIO ---
        st.subheader("1. Selecciona los Criterios para la Generaci√≥n")

        all_grades = df_datos['GRADO'].dropna().unique().tolist()
        grado_seleccionado = st.selectbox("Grado", sorted(all_grades), key="grado_sel")

        df_filtrado_grado = df_datos[df_datos['GRADO'].astype(str).str.upper() == str(grado_seleccionado).upper()]
        all_areas = df_filtrado_grado['√ÅREA'].dropna().unique().tolist()
        area_seleccionada = st.selectbox("√Årea", sorted(all_areas), key="area_sel")

        df_filtrado_area = df_filtrado_grado[df_filtrado_grado['√ÅREA'].astype(str).str.upper() == str(area_seleccionada).upper()]
        all_asignaturas = df_filtrado_area['ASIGNATURA'].dropna().unique().tolist()
        asignatura_seleccionada = st.selectbox("Asignatura", sorted(all_asignaturas), key="asignatura_sel")

        df_filtrado_asignatura = df_filtrado_area[df_filtrado_area['ASIGNATURA'].astype(str).str.upper() == str(asignatura_seleccionada).upper()]
        all_macrohabilidades = df_filtrado_asignatura['MACROHABILIDAD'].dropna().unique().tolist()
        macrohabilidad_seleccionada = st.selectbox("Macrohabilidad", sorted(all_macrohabilidades), key="macrohabilidad_sel")
    
        # --- L√ìGICA DE CARGA Y GESTI√ìN DE PROGRESO (CORREGIDA) ---
        nombre_archivo_progreso = generar_nombre_archivo_progreso(grado_seleccionado, asignatura_seleccionada, macrohabilidad_seleccionada)
        
        # Verificamos si la lista de √≠tems aprobados no existe en la sesi√≥n y la creamos vac√≠a.
        # ESTA ES LA L√çNEA CLAVE QUE SOLUCIONA EL ERROR.
        if 'approved_items' not in st.session_state:
            st.session_state['approved_items'] = []
        
        # Ahora, gestionamos la carga de progreso si el usuario cambia de macrohabilidad.
        if st.session_state.get('current_macrohabilidad') != macrohabilidad_seleccionada:
            # Si la macrohabilidad cambi√≥, cargamos el progreso desde GCS.
            # Esto reemplazar√° la lista vac√≠a con los √≠tems guardados, si existen.
            st.session_state['approved_items'] = cargar_progreso_desde_gcs(GCS_BUCKET_NAME, nombre_archivo_progreso)
            st.session_state['current_review_index'] = 0
            
        # Finalmente, actualizamos la macrohabilidad y nombre de archivo actuales en la sesi√≥n.
        st.session_state['current_macrohabilidad'] = macrohabilidad_seleccionada
        st.session_state['nombre_archivo_progreso'] = nombre_archivo_progreso
        # --- FIN DE LA L√ìGICA ---
        
        df_filtrado_macrohabilidad = df_filtrado_asignatura[df_filtrado_asignatura['MACROHABILIDAD'].astype(str).str.upper() == str(macrohabilidad_seleccionada).upper()]
        
        # --- ORDEN CORREGIDO: SECCI√ìN MOVIDA HACIA ARRIBA ---
        st.markdown("---")
        st.subheader("2. Configuraci√≥n de Modelos de Vertex AI")
        vertex_ai_models = [
            "gemini-2.5-pro",
            "gemini-2.5-flash",
            "gemini-2.5-flash-lite"
        ]
        col1, col2 = st.columns(2)
        with col1:
            gen_model_name = st.selectbox("**Modelo para Generaci√≥n**", vertex_ai_models, index=1, key="gen_vertex_name")
        with col2:
            audit_model_name = st.selectbox("**Modelo para Auditor√≠a**", vertex_ai_models, index=0, key="audit_vertex_name")
        # --- FIN DEL ORDEN CORREGIDO ---
        
        st.markdown("---")
        st.subheader("3. Selecciona las Habilidades y la Cantidad de √çtems")

        # --- INICIO DE LA NUEVA L√ìGICA DE SELECCI√ìN M√öLTIPLE ---

        # Preparamos la lista de habilidades para mostrar en la interfaz
        df_habilidades = df_filtrado_macrohabilidad.drop_duplicates(subset=['MICROHABILIDAD']).reset_index(drop=True)
        
        # Guardamos el dataframe de habilidades en el estado de la sesi√≥n para usarlo despu√©s
        st.session_state['df_habilidades_macrohabilidad'] = df_habilidades

        # Creamos un diccionario para guardar las selecciones del usuario: {indice: cantidad}
        if 'selecciones_usuario' not in st.session_state:
            st.session_state['selecciones_usuario'] = {}

        st.info("Marca las casillas de las habilidades que deseas generar y elige cu√°ntos √≠tems necesitas para cada una.")
        
        # --- C√ìDIGO DE REEMPLAZO ---
        
        # --- L√≥gica de Contexto General (Opcional y Corregida Definitivamente) ---
        contexto_general_macrohabilidad = ""
        with st.expander("üìù Opcional: Generar un contexto general para la macrohabilidad"):
            
            # 1. Inicializamos la variable de estado principal si no existe. Esta ser√° nuestra √öNICA fuente de verdad.
            if 'generated_context' not in st.session_state:
                st.session_state.generated_context = ""
            if 'show_context_refinement' not in st.session_state:
                st.session_state.show_context_refinement = False
        
            # --- WIDGETS DE SELECCI√ìN (Sin cambios) ---
            categorias_contexto = ["No usar contexto general", "Contexto Escolar", "Contexto Cotidiano", "Contexto Cient√≠fico", "Contexto Hist√≥rico", "Contexto Literario", "Contexto Pol√≠tico/Social", "Contexto Tecnol√≥gico", "Fragmento para Lectura", "Otro..."]
            categoria_elegida = st.selectbox("Elige un tipo de contexto:", categorias_contexto, key="ctx_categoria")
            tipo_contexto_final = categoria_elegida
            if categoria_elegida == "Fragmento para Lectura":
                tipos_fragmento = ["Cr√≥nica", "Noticia", "Entrevista", "Ensayo", "Cuento Corto", "Manual"]
                tipo_contexto_final = st.selectbox("Elige el tipo de fragmento:", tipos_fragmento, key="ctx_fragmento")
            elif categoria_elegida == "Otro...":
                tipo_contexto_final = st.text_input("Especifica el tipo de contexto que deseas:", key="ctx_otro", placeholder="Ej: Contexto mitol√≥gico griego")
            idea_usuario_ctx = st.text_area("Opcional: Da una idea o borrador para guiar a la IA...", key="ctx_idea", placeholder="Ej: Un equipo de bi√≥logos marinos descubre una nueva especie...")
            
            if categoria_elegida != "No usar contexto general":
                if st.button("üß† Generar Contexto con IA", key="btn_gen_ctx"):
                    with st.spinner("Generando contexto..."):
                        contexto_sugerido = generar_contexto_general_con_llm(gen_model_name, grado_seleccionado, area_seleccionada, asignatura_seleccionada, macrohabilidad_seleccionada, tipo_contexto=tipo_contexto_final, idea_usuario=idea_usuario_ctx)
                        if contexto_sugerido:
                            # Al generar, actualizamos directamente nuestra √∫nica variable de estado.
                            st.session_state.generated_context = contexto_sugerido
                            st.session_state.show_context_refinement = False
                            st.rerun()
        
            # --- EDICI√ìN Y REFINAMIENTO (L√ìGICA SIMPLIFICADA) ---
            if st.session_state.generated_context:
                st.markdown("---")
                st.markdown("##### Contexto Generado (puedes editarlo directamente):")
                
                # CAMBIO CLAVE #1: El widget ahora usa la misma clave que nuestra variable de estado.
                # Cualquier edici√≥n del usuario actualizar√° AUTOM√ÅTICAMENTE st.session_state.generated_context.
                st.text_area(
                    "Contexto generado",
                    key="generated_context",  # Esta es la clave unificada
                    height=200,
                    label_visibility="collapsed"
                )
                
                if st.button("‚úçÔ∏è Refinar Contexto con Feedback", key="btn_show_refine_ctx"):
                    st.session_state.show_context_refinement = not st.session_state.get('show_context_refinement', False)
                    st.rerun()
        
                if st.session_state.get('show_context_refinement', False):
                    with st.form("refine_context_form"):
                        feedback_ctx = st.text_area("Escribe tus observaciones para refinar:", key="ctx_feedback")
                        submitted = st.form_submit_button("üîÑ Refinar con estas Observaciones")
                        
                        if submitted and feedback_ctx:
                            # CAMBIO CLAVE #2: Leemos directamente del estado principal, que gracias al
                            # cambio #1, ya contiene las ediciones manuales del usuario.
                            contexto_base_actual = st.session_state.generated_context
                            
                            with st.spinner("Refinando contexto con tu feedback..."):
                                contexto_refinado = refinar_contexto_con_llm(
                                    gen_model_name,
                                    contexto_original=contexto_base_actual,
                                    feedback_usuario=feedback_ctx
                                )
                                
                                if contexto_refinado:
                                    # CAMBIO CLAVE #3: Solo necesitamos actualizar nuestra √∫nica variable.
                                    # El widget se actualizar√° solo en el rerun.
                                    st.session_state.generated_context = contexto_refinado
                                    st.session_state.show_context_refinement = False
                                    st.rerun()
                                else:
                                    st.error("No se pudo refinar el contexto.")
        
            # La variable final simplemente lee el estado principal, que siempre est√° actualizado.
            contexto_general_macrohabilidad = st.session_state.get('generated_context', "").strip()


        descripcion_imagen_aprobada = ""
        with st.expander("üñºÔ∏è Opcional: Usar una imagen como insumo para el √≠tem"):
            
            # Inicializamos el estado si no existe
            if 'descripcion_imagen' not in st.session_state:
                st.session_state['descripcion_imagen'] = ""
        
            uploaded_file = st.file_uploader(
                "Sube un archivo de imagen (PNG, JPG) o un PDF de una sola p√°gina",
                type=['png', 'jpg', 'jpeg', 'pdf']
            )
            
            if uploaded_file:
                # Guardamos los bytes de la imagen en la sesi√≥n para usarla despu√©s
                st.session_state['source_image_bytes'] = uploaded_file.getvalue()
                st.session_state['source_image_type'] = uploaded_file.type
            else:
                # Si no hay archivo, nos aseguramos de que no haya una imagen vieja en memoria
                if 'source_image_bytes' in st.session_state:
                    del st.session_state['source_image_bytes']
                if 'source_image_type' in st.session_state:
                    del st.session_state['source_image_type']
                    
            if uploaded_file is not None:
                # Bot√≥n para activar el an√°lisis
                if st.button("üß† Analizar y Describir Imagen"):
                    with st.spinner("Analizando la imagen con IA..."):
                        file_bytes = uploaded_file.getvalue()
                        mime_type = uploaded_file.type
                        
                        # Usamos un modelo multimodal (Pro es ideal para esto)
                        descripcion_generada = describir_imagen_con_llm(
                            "gemini-2.5-flash", # O el modelo multimodal que prefieras
                            file_bytes, 
                            mime_type
                        )
                        
                        if descripcion_generada:
                            st.session_state['descripcion_imagen'] = descripcion_generada
                            st.rerun() # Refresca para mostrar el text_area
        
            # Si ya hay una descripci√≥n generada, la mostramos para edici√≥n
            if st.session_state['descripcion_imagen']:
                st.markdown("##### Descripci√≥n Generada (puedes editarla):")
                
                edited_description = st.text_area(
                    "Edita la descripci√≥n si es necesario:",
                    value=st.session_state['descripcion_imagen'],
                    height=250,
                    key="desc_img_edited"
                )
                
                # El texto final que se usar√° es el que est√© en el √°rea de texto
                descripcion_imagen_aprobada = edited_description.strip()
                st.success("‚úÖ La descripci√≥n est√° lista para ser usada en la generaci√≥n del √≠tem.")
        
        st.markdown("---") # Separador visual

        # Creamos la interfaz interactiva para la selecci√≥n
        for index, row in df_habilidades.iterrows():
            proceso = row['PROCESO COGNITIVO']
            micro = row['MICROHABILIDAD']
            label = f"**{proceso}** // {micro}"
            
            # Usamos el √≠ndice como identificador √∫nico
            is_checked = st.checkbox(label, key=f"cb_{index}")

            if is_checked:
                # Si est√° marcado, mostramos el selector de cantidad y guardamos la elecci√≥n
                cantidad = st.selectbox(
                    "Cantidad de √≠tems:",
                    options=[1, 2, 3],
                    key=f"qty_{index}",
                    label_visibility="collapsed"
                )
                st.session_state['selecciones_usuario'][index] = cantidad
            elif index in st.session_state['selecciones_usuario']:
                # Si se desmarca, lo eliminamos de las selecciones
                del st.session_state['selecciones_usuario'][index]

        # Definimos df_item_seleccionado como el dataframe completo para que la validaci√≥n posterior funcione
        # La l√≥gica real de selecci√≥n se basa en 'selecciones_usuario'
        df_item_seleccionado = df_filtrado_macrohabilidad.copy()
        
        # --- FIN DE LA NUEVA L√ìGICA DE SELECCI√ìN M√öLTIPLE ---
        
        if df_item_seleccionado is None or df_item_seleccionado.empty:
            st.error("No hay datos v√°lidos para generar √≠tems con los filtros actuales.")
        else:
            # --- El c√≥digo de prompts y el bot√≥n ahora est√°n dentro del 'else' ---
            st.markdown("---")
            st.subheader("4. Personaliza con Prompts Adicionales (Opcional)")
            prompt_bloom_adicional, prompt_construccion_adicional, prompt_especifico_adicional, prompt_auditor_adicional = "", "", "", ""
            if st.checkbox("Activar Prompts Adicionales"):
                st.info("Estos prompts se a√±adir√°n a las instrucciones de la IA para un control m√°s fino.")
                prompt_bloom_adicional = st.text_area("Prompt para Taxonom√≠a de Bloom:", help="Ej: 'Aseg√∫rate que la pregunta requiera que el estudiante compare dos conceptos...'")
                prompt_construccion_adicional = st.text_area("Prompt para Construcci√≥n de √çtem:", help="Ej: 'Usa un lenguaje formal y evita coloquialismos.'")
                prompt_especifico_adicional = st.text_area("Prompt para Consideraciones Espec√≠ficas:", help="Ej: 'El contexto debe estar relacionado con la ecolog√≠a de un bosque.'")
                prompt_auditor_adicional = st.text_area("Prompt para el Auditor:", help="Ej: 'Verifica que la dificultad sea adecuada para un examen final.'")

            st.markdown("---")
            
            # =============================================================================
            # BLOQUE 1 MODIFICADO: L√ìGICA DEL BOT√ìN PRINCIPAL
            # =============================================================================
            if st.button("üöÄ Generar y Auditar √çtem(s)"):
                if not st.session_state.get('selecciones_usuario'):
                    st.warning("‚ö†Ô∏è Por favor, selecciona al menos una habilidad para generar √≠tems.")
                else:
                    criterios_para_preguntas = {
                        "tipo_pregunta": "opci√≥n m√∫ltiple con 4 opciones",
                        "dificultad": "media",
                        "contexto_educativo": "estudiantes Colombianos entre 10 y 17 a√±os",
                    }

                    # --- Construimos la nueva "cola de tareas" basada en la selecci√≥n del usuario ---
                    items_para_procesar = []
                    df_habilidades_macrohabilidad = st.session_state['df_habilidades_macrohabilidad']

                    for index, cantidad in st.session_state.selecciones_usuario.items():
                        habilidad_seleccionada = df_habilidades_macrohabilidad.loc[index].to_dict()
                        for _ in range(cantidad):
                            items_para_procesar.append(habilidad_seleccionada)
                    
                    if items_para_procesar:
                        st.session_state.items_para_procesar = items_para_procesar
                        st.session_state.current_review_index = 0
                        st.session_state.awaiting_review = True
                        st.session_state.modo_lote = True
                        st.session_state.selecciones_usuario = {}
                        st.rerun()
            
            # =============================================================================
            # BLOQUE 2 MODIFICADO: SECCI√ìN DE REVISI√ìN CON GENERACI√ìN SECUENCIAL Y GR√ÅFICOS
            # =============================================================================
            if 'awaiting_review' in st.session_state and st.session_state['awaiting_review']:
                
                current_index = st.session_state.get('current_review_index', 0)
                
                # --- L√ìGICA DE CONTROL CON SESSION_STATE ---
                items_pendientes = st.session_state.get('items_para_procesar', [])
                total_items = len(items_pendientes)
                
                if current_index >= total_items:
                    st.session_state['awaiting_review'] = False
                    if 'item_under_review' in st.session_state:
                        del st.session_state['item_under_review']
                    st.rerun()
                else:
                    # Intentamos obtener el √≠tem de la memoria de la sesi√≥n
                    item_to_review = st.session_state.get('item_under_review')
                    
                    # Si NO hay un √≠tem en memoria, lo generamos
                    if item_to_review is None:
                        # 1. Crea un marcador de posici√≥n que ocupar√° un espacio en la pantalla
                        placeholder = st.empty()
            
                        # 2. Carga y muestra la animaci√≥n DENTRO del marcador de posici√≥n
                        lottie_url = "https://lottie.host/41f1128a-22f4-40ad-99c8-1076328efb3e/MMre1fyJsg.json" # URL del dinosaurio
                        lottie_json = load_lottieurl(lottie_url)
            
                        with placeholder.container():
                            st.subheader(f"üìù Generando y Revisando √çtem ({current_index + 1} de {total_items})")
                            if lottie_json:
                                st_lottie(lottie_json, height=200, key="lottie_loading")
                            else:
                                st.info("Cargando animaci√≥n...") # Mensaje de respaldo
            
                        # 3. EJECUTA TU PROCESO LARGO (La generaci√≥n de la IA)
                        item_spec_row = items_pendientes[current_index]
                        current_fila_datos = {
                            'GRADO': grado_seleccionado, '√ÅREA': area_seleccionada, 'ASIGNATURA': asignatura_seleccionada, 'MACROHABILIDAD': macrohabilidad_seleccionada,
                            **item_spec_row
                        }
                        criterios_para_preguntas = {"tipo_pregunta": "opci√≥n m√∫ltiple con 4 opciones", "dificultad": "media", "contexto_educativo": "estudiantes Colombianos entre 10 y 17 a√±os"}
                      
                        # --- A√ëADIR ESTE BLOQUE DE B√öSQUEDA ---
                        contexto_del_libro = ""
                        if 'pdf_index' in st.session_state:
                            with st.spinner("Buscando en el libro gu√≠a..."):
                                # Usamos la microhabilidad como consulta
                                query_microhabilidad = current_fila_datos.get('MICROHABILIDAD', '')
                                
                                # Buscamos los 3 chunks m√°s relevantes
                                chunks_relevantes = buscar_en_indice(query_microhabilidad, k=3)
                                
                                if chunks_relevantes:
                                    contexto_del_libro = "\n\n---\n\n".join(chunks_relevantes)
                                    st.info("‚ÑπÔ∏è Contexto relevante encontrado en el libro gu√≠a.")
                        # --- FIN DEL BLOQUE DE B√öSQUEDA ---

    
                        item_to_review = generar_pregunta_con_seleccion(
                            gen_model_name, audit_model_name, fila_datos=current_fila_datos,
                            criterios_generacion=criterios_para_preguntas, manual_reglas_texto=manual_reglas_texto,
                            contexto_general_macrohabilidad=contexto_general_macrohabilidad,
                            prompt_bloom_adicional=prompt_bloom_adicional, prompt_construccion_adicional=prompt_construccion_adicional,
                            prompt_especifico_adicional=prompt_especifico_adicional, prompt_auditor_adicional=prompt_auditor_adicional, descripcion_imagen_aprobada=descripcion_imagen_aprobada
                        )
                        st.session_state['item_under_review'] = item_to_review
                        
                        # 4. Una vez que el proceso termina, limpia el marcador de posici√≥n
                        placeholder.empty()
            
                    # Si se gener√≥ o encontr√≥ un √≠tem, lo mostramos para revisi√≥n
                    if item_to_review:
                        with st.expander("Ver detalles de clasificaci√≥n del √≠tem", expanded=False):
                            st.json(item_to_review['classification'])
                        
                        st.markdown("##### √çtem Generado:")
                        st.text_area("√çtem", value=item_to_review['item_text'], height=400, key=f"item_text_{current_index}", disabled=True)
                        
                        st.markdown("##### Resultado de la Auditor√≠a:")
                        status = item_to_review['final_audit_status']
                        if "‚úÖ" in status:
                            st.success(f"**Dictamen:** {status}")
                        elif "‚ö†Ô∏è" in status:
                            st.warning(f"**Dictamen:** {status}")
                        else:
                            st.error(f"**Dictamen:** {status}")
                        st.markdown(f"**Observaciones:** {item_to_review['final_audit_observations']}")

                        # =============================================================================
                        # --- INICIO: INTEGRACI√ìN DEL GENERADOR DE GR√ÅFICOS (MODIFICADO) ---
                        # =============================================================================

                        if item_to_review.get("grafico_necesario") == 'S√ç':
                            # Obtenemos la LISTA de descripciones que ya procesamos en el paso 2
                            descripciones = item_to_review.get("descripciones_graficos", [])
                            
                            # Usamos un expander de Streamlit para mostrar todos los gr√°ficos de forma ordenada
                            with st.expander(f"üé® Gr√°ficos Requeridos para este √çtem ({len(descripciones)})", expanded=True):
                                
                                if not descripciones:
                                    st.warning("El √≠tem indica que requiere gr√°ficos, pero no se encontr√≥ una descripci√≥n v√°lida.")
                                else:
                                    # Iteramos sobre cada descripci√≥n de gr√°fico en la lista
                                    for idx, desc_grafico in enumerate(descripciones):
                                        
                                        # Extraemos la ubicaci√≥n para etiquetar cada secci√≥n del gr√°fico
                                        ubicacion = desc_grafico.get("ubicacion", f"Gr√°fico #{idx + 1}").replace("_", " ").title()
                                        st.markdown(f"--- \n**Gr√°fico para:** `{ubicacion}`")
                        
                                        # Convertimos el diccionario del gr√°fico a un string JSON formateado para mostrarlo y editarlo.
                                        # 'ensure_ascii=False' es importante para que muestre bien las tildes y caracteres en espa√±ol.
                                        descripcion_actual_str = json.dumps(desc_grafico, indent=2, ensure_ascii=False)
                                        
                                        # Clave √öNICA para cada widget, ¬°esto es muy importante para que Streamlit funcione bien en bucles!
                                        key_base = f"chart_{current_index}_{idx}"
                        
                                        edited_description = st.text_area(
                                            "Descripci√≥n JSON del Gr√°fico (puedes editarla):",
                                            value=descripcion_actual_str,
                                            key=f"desc_{key_base}", # Clave √∫nica para el √°rea de texto
                                            height=200
                                        )
                        
                                        if st.button("üñºÔ∏è Generar / Actualizar Gr√°fico", key=f"btn_{key_base}"):
                                            if edited_description:
                                                
                                                # --- INICIO DE LA L√ìGICA INTELIGENTE ---
                                                buffer_imagen = None
                                                
                                                # 1. Intentamos leer el texto como un JSON estructurado.
                                                try:
                                                    datos_grafico = json.loads(edited_description)
                                                    tipo_elemento = datos_grafico.get("tipo_elemento")
                                                except json.JSONDecodeError:
                                                    # Si falla (porque es texto simple), es un trabajo para el "Artista".
                                                    tipo_elemento = "otro_tipo"
                                                    datos_grafico = {} # Creamos un dict vac√≠o para evitar errores

                                                # 2. Decidimos qu√© funci√≥n llamar.
                                                if tipo_elemento == "otro_tipo":
                                                    # Es un trabajo creativo, llamamos al Artista (IA generativa).
                                                    with st.spinner("ü§ñ Creando visualizaci√≥n con IA generativa..."):
                                                        # Usamos la descripci√≥n natural del JSON o el texto completo si no es JSON.
                                                        prompt_para_imagen = datos_grafico.get("datos", {}).get("descripcion_natural", edited_description)
                                                        buffer_imagen = generar_imagen_con_ia(prompt_para_imagen)
                                                else:
                                                    # Es un trabajo de datos, llamamos al Ingeniero (el plugin de gr√°ficos).
                                                    with st.spinner("‚öôÔ∏è Construyendo gr√°fico desde datos..."):
                                                        _, buffer_imagen = generar_grafico_desde_texto(descripcion=edited_description)
                                                        
                                                # --- FIN DE LA L√ìGICA INTELIGENTE ---

                                                # 3. Guardamos el resultado en la sesi√≥n.
                                                if buffer_imagen:
                                                    st.session_state[f'img_{key_base}'] = buffer_imagen
                                                    st.session_state[f'caption_{key_base}'] = f"Gr√°fico para '{ubicacion}' generado."
                                                else:
                                                    st.session_state[f'img_{key_base}'] = None
                                                    st.session_state[f'caption_{key_base}'] = "No se pudo generar el gr√°fico con la descripci√≥n proporcionada."

                                                # Forzamos un refresco para mostrar el resultado inmediatamente.
                                                st.rerun()
                                            else:
                                                st.warning("La descripci√≥n est√° vac√≠a. No se puede generar el gr√°fico.")
                                        
                                        # Mostramos la imagen si ya fue generada y guardada en la sesi√≥n.
                                        if f'img_{key_base}' in st.session_state and st.session_state[f'img_{key_base}']:
                                            st.image(
                                                st.session_state[f'img_{key_base}'],
                                                caption=st.session_state.get(f'caption_{key_base}'),
                                                use_column_width=True
                                            )
                                        # Si no hay imagen pero s√≠ un mensaje (porque fall√≥), lo mostramos.
                                        elif f'caption_{key_base}' in st.session_state:
                                            st.warning(st.session_state[f'caption_{key_base}'])
                                            
                        # =============================================================================
                        # --- FIN: NUEVA INTERFAZ PARA M√öLTIPLES GR√ÅFICOS ---
                        # =============================================================================
                        # =============================================================================
                        # BLOQUE 3: L√ìGICA DE BOTONES (VERIFICADA, CON CAMBIOS NECESARIOS)
                        # =============================================================================
                        col_aprob, col_rechazo, col_descartar = st.columns(3)
            
                        with col_aprob:
                            if st.button("üëç Aprobar y Siguiente", key=f"approve_{current_index}", use_container_width=True):
                                
                                # --- INICIO: NUEVA L√ìGICA PARA CAPTURAR IM√ÅGENES ---
                                if 'source_image_bytes' in st.session_state:
                                        item_to_review['source_image'] = io.BytesIO(st.session_state['source_image_bytes'])
                                if item_to_review.get("grafico_necesario") == 'S√ç':
                                    generated_images = []
                                    descripciones = item_to_review.get("descripciones_graficos", [])
                                    for idx, desc in enumerate(descripciones):
                                        key_base = f"chart_{current_index}_{idx}"
                                        # Buscamos si la imagen existe en la sesi√≥n
                                        if f'img_{key_base}' in st.session_state and st.session_state[f'img_{key_base}']:
                                            generated_images.append({
                                                "ubicacion": desc.get("ubicacion"),
                                                "buffer": st.session_state[f'img_{key_base}']
                                            })
                                    # A√±adimos la lista de im√°genes capturadas al √≠tem
                                    item_to_review['generated_images'] = generated_images
                                # --- FIN: NUEVA L√ìGICA ---
                        
                                st.session_state.approved_items.append(item_to_review)
                                guardar_progreso_en_gcs(GCS_BUCKET_NAME, st.session_state.nombre_archivo_progreso, st.session_state.approved_items)
                                
                                # ... (el resto del c√≥digo para limpiar la sesi√≥n y hacer rerun se mantiene igual) ...
                                # Limpiamos las im√°genes de la sesi√≥n para el siguiente √≠tem
                                for idx, desc in enumerate(item_to_review.get("descripciones_graficos", [])):
                                    key_base = f"chart_{current_index}_{idx}"
                                    if f'img_{key_base}' in st.session_state: del st.session_state[f'img_{key_base}']
                                    if f'caption_{key_base}' in st.session_state: del st.session_state[f'caption_{key_base}']
                        
                                st.session_state.current_review_index += 1
                                st.session_state['show_feedback_form'] = False
                                st.session_state['item_under_review'] = None
                                st.rerun()
                                
                        with col_rechazo:
                            if st.button("‚úçÔ∏è Refinar con Feedback", key=f"refine_{current_index}", use_container_width=True):
                                # Hacemos el cambio de estado de forma m√°s expl√≠cita
                                if 'show_feedback_form' not in st.session_state:
                                    st.session_state.show_feedback_form = True
                                else:
                                    st.session_state.show_feedback_form = not st.session_state.show_feedback_form
                                
                                # Forzamos un rerun para asegurar que la app se actualice con el nuevo estado
                                st.rerun()            
            
                        with col_descartar:
                            if st.button("üëé Descartar √çtem", key=f"discard_{current_index}", use_container_width=True):
                                if f'generated_chart_image_{current_index}' in st.session_state:
                                    del st.session_state[f'generated_chart_image_{current_index}']
                                if f'generated_chart_caption_{current_index}' in st.session_state:
                                    del st.session_state[f'generated_chart_caption_{current_index}']
                                if f'chart_description_{current_index}' in st.session_state:
                                    del st.session_state[f'chart_description_{current_index}']                   
                                st.session_state.current_review_index += 1
                                st.session_state['show_feedback_form'] = False
                                st.session_state['item_under_review'] = None # Limpiamos para generar el siguiente
                                st.rerun()
                        
                        # Formulario de Feedback
                        if st.session_state.get('show_feedback_form', False):
                            with st.form(key='feedback_form'):
                                st.markdown("---")
                                st.markdown("#### Proporciona tus observaciones para refinar el √≠tem:")
                                feedback_usuario = st.text_area(
                                    "Escribe aqu√≠ tus correcciones o sugerencias...",
                                    key="feedback_text"
                                )
                                submitted = st.form_submit_button("üîÑ Refinar con estas Observaciones")
                        
                                if submitted and feedback_usuario:
                                    # 1. Recuperamos el √≠tem actual desde el session_state AHORA MISMO.
                                    item_actual_para_refinar = st.session_state.get('item_under_review')
                        
                                    # 2. Verificamos que el √≠tem realmente existe antes de continuar.
                                    if item_actual_para_refinar and 'item_text' in item_actual_para_refinar:
                                        with st.spinner("üß† Refinando el √≠tem con tu feedback..."):
                                            classif_norm = normaliza_claves_classif(item_actual_para_refinar.get('classification', {}))
                                            
                                            # 3. Llamamos a la funci√≥n con los datos correctos y verificados.
                                            refined_item_data = generar_pregunta_con_seleccion(
                                                gen_model_name=st.session_state.gen_vertex_name,
                                                audit_model_name=st.session_state.audit_vertex_name,
                                                fila_datos=classif_norm,
                                                criterios_generacion={
                                                    "tipo_pregunta": "opci√≥n m√∫ltiple con 4 opciones", "dificultad": "media",
                                                    "contexto_educativo": "estudiantes Colombianos entre 10 y 17 a√±os"
                                                },
                                                manual_reglas_texto=manual_reglas_texto,
                                                feedback_usuario=feedback_usuario,
                                                # Usamos el texto del √≠tem que acabamos de recuperar.
                                                item_a_refinar_text=item_actual_para_refinar['item_text']
                                            )
                        
                                            if refined_item_data:
                                                st.session_state['item_under_review'] = refined_item_data
                                                st.session_state['show_feedback_form'] = False
                                                st.success("¬°√çtem refinado! Por favor, rev√≠salo de nuevo.")
                                                st.rerun()
                                            else:
                                                st.error("Fallo al refinar el √≠tem. Intenta de nuevo o ajusta tu feedback.")
                                    else:
                                        # Si por alguna raz√≥n el √≠tem se perdi√≥, informamos al usuario.
                                        st.error("Error de estado: No se encontr√≥ el √≠tem a refinar. Por favor, descarte este √≠tem y genere uno nuevo.")

                                    
                                    if refined_item_data:
                                        # Reemplazamos el √≠tem en revisi√≥n con la versi√≥n refinada
                                        st.session_state['item_under_review'] = refined_item_data
                                        st.session_state['show_feedback_form'] = False
                                        st.success("¬°√çtem refinado! Por favor, rev√≠salo de nuevo.")
                                        st.rerun()
                                    else:
                                        st.error("Fallo al refinar el √≠tem. Intenta de nuevo o ajusta tu feedback.")

            if 'approved_items' in st.session_state and st.session_state['approved_items']:
                if not st.session_state.get('awaiting_review', False):
                    st.markdown("---")
                    st.subheader(f"‚úÖ √çtems Aprobados: {len(st.session_state.approved_items)}")
                    st.success("Todos los √≠tems seleccionados han sido procesados. Ahora puedes exportarlos.")

                    nombre_archivo_zip = f"items_{macrohabilidad_seleccionada.replace(' ', '_')}.zip"
                    
                    zip_buffer = exportar_a_zip(st.session_state.approved_items)
                    st.download_button(
                        label="üì• Descargar todos los √çtems Aprobados (.zip)",
                        data=zip_buffer,
                        file_name=nombre_archivo_zip,
                        mime="application/zip",
                        use_container_width=True
                    )
                                        
                    st.write("")
                    nombre_base = macrohabilidad_seleccionada.replace(' ', '_').lower()
                    excel_buffer = exportar_a_excel(st.session_state.approved_items, nombre_base)
                    nombre_archivo_excel = f"items_aprobados_{nombre_base}.xlsx"
                    if excel_buffer:
                        st.download_button(
                            label="üì• Descargar √çtems Aprobados (.xlsx)",
                            data=excel_buffer,
                            file_name=nombre_archivo_excel,
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True
                        )
                    
                    with st.expander("Ver y Descargar Prompts Utilizados"):
                        st.info("Aqu√≠ puedes descargar un archivo de texto con los prompts completos que se enviaron a la IA para generar y auditar los √≠tems aprobados.")
                        
                        combined_prompts_content = ""
                        for i, item_data in enumerate(st.session_state['approved_items']):
                            combined_prompts_content += f"--- √çTEM APROBADO #{i+1} ---\n"
                            combined_prompts_content += f"Clasificaci√≥n: {item_data.get('classification', {})}\n"
                            combined_prompts_content += "="*40 + "\n\n"
                            combined_prompts_content += f"--- PROMPT DE GENERACI√ìN ---\n"
                            combined_prompts_content += f"{item_data.get('generation_prompt_used', 'No disponible')}\n\n"
                            combined_prompts_content += f"--- PROMPT DE AUDITOR√çA ---\n"
                            combined_prompts_content += f"{item_data.get('auditor_prompt_used', 'No disponible')}\n\n"
                            combined_prompts_content += "#"*80 + "\n\n"
                        
                        st.download_button(
                            label="üì• Descargar Prompts (.txt)",
                            data=combined_prompts_content.encode('utf-8'),
                            file_name=f"prompts_{macrohabilidad_seleccionada.replace(' ', '_')}.txt",
                            mime="text/plain",
                            use_container_width=True
                        )
                    
                    st.markdown("---")
                    if st.button("‚ú® Reset: Borrar informaci√≥n y generar nuevo √≠tem", use_container_width=True, type="primary"):
                        
                        # --- L√çNEAS A√ëADIDAS PARA BORRAR PROGRESO ---
                        if 'nombre_archivo_progreso' in st.session_state:
                            borrar_progreso_en_gcs(GCS_BUCKET_NAME, st.session_state.nombre_archivo_progreso)
                        # ----------------------------------------------
                    
                        # Limpiar todos los estados relevantes
                        
                        keys_to_pop = ['approved_items', 'processed_items_list_for_review', 'current_review_index',
                                       'awaiting_review', 'items_para_procesar', 'modo_lote', 'show_feedback_form',
                                       'context_approved', 'generated_context', 'show_context_options', 'nombre_archivo_progreso',
                                       'source_image_bytes', 'source_image_type', 'descripcion_imagen']

                        for key in keys_to_pop:
                            if key in st.session_state:
                                st.session_state.pop(key)
                        
                        st.rerun()

# --- BLOQUE DE EJECUCI√ìN ---
if __name__ == "__main__":
    load_dotenv() # Carga las variables de entorno

    # --- AUTENTICACI√ìN SEGURA CON VARIABLES DE ENTORNO ---
    # Lee la contrase√±a desde los secretos configurados en la plataforma de despliegue
    PASSWORD = os.environ.get("PASSWORD")

    if "authenticated" not in st.session_state:
        st.session_state["authenticated"] = False

    if not st.session_state["authenticated"]:
        st.set_page_config(layout="centered")
        st.title("üîí Acceso restringido")
        pwd = st.text_input("Introduce la contrase√±a:", type="password")

        if st.button("Entrar"):
            if pwd and pwd == PASSWORD:
                st.session_state["authenticated"] = True
                st.rerun()
            else:
                st.error("‚ùå Contrase√±a incorrecta")
        st.stop()
    

    # --- SI LA CONTRASE√ëA ES CORRECTA, EJECUTA LA APP ---
    try:
        main()
    except Exception as e:
        st.set_page_config(layout="centered")
        st.title("üõë Error Cr√≠tico de la Aplicaci√≥n")
        st.error(
            "Ocurri√≥ un error grave que impidi√≥ que la aplicaci√≥n se iniciara correctamente. "
            "Esto suele ser un problema de configuraci√≥n o permisos en Google Cloud."
        )
        st.subheader("Mensaje de Error T√©cnico:")
        st.exception(e)
